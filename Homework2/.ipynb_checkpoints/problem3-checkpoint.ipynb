{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import torch\n",
    "#print(torch.__version__)\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch.nn.init\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>V</th>\n",
       "      <th>M</th>\n",
       "      <th>n</th>\n",
       "      <th>np</th>\n",
       "      <th>B</th>\n",
       "      <th>G</th>\n",
       "      <th>E</th>\n",
       "      <th>v</th>\n",
       "      <th>H</th>\n",
       "      <th>...</th>\n",
       "      <th>Formula</th>\n",
       "      <th>space group</th>\n",
       "      <th>k_BTE_DFT</th>\n",
       "      <th>k_BTE_DFT（1000 K）</th>\n",
       "      <th>k_AFLOW_AAPL</th>\n",
       "      <th>k_AFLOW_AGL</th>\n",
       "      <th>k_AFLOW_AGL (Poisson ratio σ=0.25)</th>\n",
       "      <th>k_Mingo</th>\n",
       "      <th>k_EXP</th>\n",
       "      <th>k_EXP（1000 K）</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66_C</td>\n",
       "      <td>11.41</td>\n",
       "      <td>24.022</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>431.7450</td>\n",
       "      <td>518.2490</td>\n",
       "      <td>1110.438653</td>\n",
       "      <td>0.071337</td>\n",
       "      <td>95.902307</td>\n",
       "      <td>...</td>\n",
       "      <td>C (diamond)</td>\n",
       "      <td>Fd-3m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2270.00</td>\n",
       "      <td>419.90</td>\n",
       "      <td>169.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2200/3000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20012_InSb</td>\n",
       "      <td>73.73</td>\n",
       "      <td>236.570</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>36.8242</td>\n",
       "      <td>14.5179</td>\n",
       "      <td>38.494849</td>\n",
       "      <td>0.325772</td>\n",
       "      <td>3.219550</td>\n",
       "      <td>...</td>\n",
       "      <td>InSb</td>\n",
       "      <td>F-43m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.02</td>\n",
       "      <td>3.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20/16.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1190_ZnSe</td>\n",
       "      <td>47.34</td>\n",
       "      <td>144.350</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>56.3189</td>\n",
       "      <td>29.0179</td>\n",
       "      <td>74.293904</td>\n",
       "      <td>0.280139</td>\n",
       "      <td>6.603032</td>\n",
       "      <td>...</td>\n",
       "      <td>ZnSe</td>\n",
       "      <td>F-43m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.44</td>\n",
       "      <td>7.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19/33</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>682_NaF</td>\n",
       "      <td>24.74</td>\n",
       "      <td>41.988</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>52.4118</td>\n",
       "      <td>36.3275</td>\n",
       "      <td>88.528881</td>\n",
       "      <td>0.218483</td>\n",
       "      <td>10.653966</td>\n",
       "      <td>...</td>\n",
       "      <td>NaF</td>\n",
       "      <td>Fm-3m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.11</td>\n",
       "      <td>4.67</td>\n",
       "      <td>4.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.5/18.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16220_Si</td>\n",
       "      <td>40.97</td>\n",
       "      <td>56.170</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>90.0009</td>\n",
       "      <td>63.4097</td>\n",
       "      <td>154.050571</td>\n",
       "      <td>0.214724</td>\n",
       "      <td>15.043345</td>\n",
       "      <td>...</td>\n",
       "      <td>Si</td>\n",
       "      <td>Fd-3m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144.00</td>\n",
       "      <td>26.19</td>\n",
       "      <td>20.58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>146/166</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Name      V        M  n  np         B         G            E  \\\n",
       "0        66_C  11.41   24.022  2   2  431.7450  518.2490  1110.438653   \n",
       "1  20012_InSb  73.73  236.570  2   2   36.8242   14.5179    38.494849   \n",
       "2   1190_ZnSe  47.34  144.350  2   2   56.3189   29.0179    74.293904   \n",
       "3     682_NaF  24.74   41.988  2   2   52.4118   36.3275    88.528881   \n",
       "4    16220_Si  40.97   56.170  2   2   90.0009   63.4097   154.050571   \n",
       "\n",
       "          v          H  ...      Formula  space group  k_BTE_DFT  \\\n",
       "0  0.071337  95.902307  ...  C (diamond)        Fd-3m        NaN   \n",
       "1  0.325772   3.219550  ...         InSb        F-43m        NaN   \n",
       "2  0.280139   6.603032  ...         ZnSe        F-43m        NaN   \n",
       "3  0.218483  10.653966  ...          NaF        Fm-3m        NaN   \n",
       "4  0.214724  15.043345  ...           Si        Fd-3m        NaN   \n",
       "\n",
       "   k_BTE_DFT（1000 K）  k_AFLOW_AAPL  k_AFLOW_AGL  \\\n",
       "0                NaN       2270.00       419.90   \n",
       "1                NaN           NaN         3.02   \n",
       "2                NaN           NaN         5.44   \n",
       "3                NaN         21.11         4.67   \n",
       "4                NaN        144.00        26.19   \n",
       "\n",
       "   k_AFLOW_AGL (Poisson ratio σ=0.25)  k_Mingo      k_EXP  k_EXP（1000 K）  \n",
       "0                              169.10      NaN  2200/3000            NaN  \n",
       "1                                3.64      NaN    20/16.5            NaN  \n",
       "2                                7.46      NaN      19/33            NaN  \n",
       "3                                4.52      NaN  16.5/18.4            NaN  \n",
       "4                               20.58      NaN    146/166            NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('thermal-dataset.xlsx', sheet_name ='dataset')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Name', 'V', 'M', 'n', 'np', 'B', 'G', 'E', 'v', 'H', 'B'', 'G'', 'ρ',\n",
       "       'vL', 'vS', 'va', 'Θe', 'γel', 'γes', 'γe', 'A', 'y-exp', 'y-theory',\n",
       "       'Formula', 'space group', 'k_BTE_DFT', 'k_BTE_DFT（1000 K）',\n",
       "       'k_AFLOW_AAPL', 'k_AFLOW_AGL', 'k_AFLOW_AGL (Poisson ratio σ=0.25)',\n",
       "       'k_Mingo', 'k_EXP', 'k_EXP（1000 K）'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Name', 'V', 'M', 'n', 'np', 'B', 'G', 'E', 'v', 'H', 'B'', 'G'', 'ρ',\n",
       "       'vL', 'vS', 'va', 'Θe', 'γel', 'γes', 'γe', 'A', 'y-exp', 'y-theory'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[:, :'y-theory']\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V</th>\n",
       "      <th>M</th>\n",
       "      <th>n</th>\n",
       "      <th>np</th>\n",
       "      <th>B</th>\n",
       "      <th>G</th>\n",
       "      <th>E</th>\n",
       "      <th>v</th>\n",
       "      <th>H</th>\n",
       "      <th>B'</th>\n",
       "      <th>...</th>\n",
       "      <th>vL</th>\n",
       "      <th>vS</th>\n",
       "      <th>va</th>\n",
       "      <th>Θe</th>\n",
       "      <th>γel</th>\n",
       "      <th>γes</th>\n",
       "      <th>γe</th>\n",
       "      <th>A</th>\n",
       "      <th>y-exp</th>\n",
       "      <th>y-theory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>3.700000e+02</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>145.503405</td>\n",
       "      <td>576.941738</td>\n",
       "      <td>6.810811</td>\n",
       "      <td>3.789189</td>\n",
       "      <td>111.257870</td>\n",
       "      <td>59.678087</td>\n",
       "      <td>147.105572</td>\n",
       "      <td>0.296581</td>\n",
       "      <td>10.521358</td>\n",
       "      <td>-6.982807</td>\n",
       "      <td>...</td>\n",
       "      <td>5.385817</td>\n",
       "      <td>2.928926</td>\n",
       "      <td>3.256445</td>\n",
       "      <td>263.253037</td>\n",
       "      <td>1.594866</td>\n",
       "      <td>1.119608</td>\n",
       "      <td>1.995672</td>\n",
       "      <td>9.121067e-05</td>\n",
       "      <td>44.079546</td>\n",
       "      <td>24.255426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>132.334810</td>\n",
       "      <td>625.505406</td>\n",
       "      <td>4.818313</td>\n",
       "      <td>3.150066</td>\n",
       "      <td>82.433063</td>\n",
       "      <td>77.165080</td>\n",
       "      <td>170.936177</td>\n",
       "      <td>0.083493</td>\n",
       "      <td>14.299973</td>\n",
       "      <td>11.632497</td>\n",
       "      <td>...</td>\n",
       "      <td>2.784979</td>\n",
       "      <td>1.973751</td>\n",
       "      <td>2.152686</td>\n",
       "      <td>225.042124</td>\n",
       "      <td>2.868737</td>\n",
       "      <td>4.816583</td>\n",
       "      <td>3.987999</td>\n",
       "      <td>9.802296e-04</td>\n",
       "      <td>277.377084</td>\n",
       "      <td>49.038768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>11.410000</td>\n",
       "      <td>8.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.904080</td>\n",
       "      <td>0.406580</td>\n",
       "      <td>1.215183</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.006197</td>\n",
       "      <td>-104.589000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.559351</td>\n",
       "      <td>0.331246</td>\n",
       "      <td>0.379108</td>\n",
       "      <td>25.916963</td>\n",
       "      <td>-7.029521</td>\n",
       "      <td>-65.475244</td>\n",
       "      <td>0.414990</td>\n",
       "      <td>1.459528e-07</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>0.026157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>53.210000</td>\n",
       "      <td>157.831500</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>47.720850</td>\n",
       "      <td>19.017575</td>\n",
       "      <td>49.639698</td>\n",
       "      <td>0.250683</td>\n",
       "      <td>3.580389</td>\n",
       "      <td>-8.717083</td>\n",
       "      <td>...</td>\n",
       "      <td>3.836785</td>\n",
       "      <td>1.889592</td>\n",
       "      <td>2.115443</td>\n",
       "      <td>144.201287</td>\n",
       "      <td>1.107011</td>\n",
       "      <td>0.624775</td>\n",
       "      <td>0.921418</td>\n",
       "      <td>9.899596e-07</td>\n",
       "      <td>3.887500</td>\n",
       "      <td>4.432038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>76.590000</td>\n",
       "      <td>287.557000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>92.003700</td>\n",
       "      <td>44.813700</td>\n",
       "      <td>114.970283</td>\n",
       "      <td>0.303591</td>\n",
       "      <td>7.326539</td>\n",
       "      <td>-2.906780</td>\n",
       "      <td>...</td>\n",
       "      <td>4.836986</td>\n",
       "      <td>2.496888</td>\n",
       "      <td>2.782292</td>\n",
       "      <td>210.531759</td>\n",
       "      <td>1.379069</td>\n",
       "      <td>1.027563</td>\n",
       "      <td>1.199428</td>\n",
       "      <td>1.864054e-06</td>\n",
       "      <td>11.075000</td>\n",
       "      <td>12.781238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>219.020000</td>\n",
       "      <td>907.187000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>160.493000</td>\n",
       "      <td>67.233975</td>\n",
       "      <td>175.339469</td>\n",
       "      <td>0.344524</td>\n",
       "      <td>11.691259</td>\n",
       "      <td>-1.296300</td>\n",
       "      <td>...</td>\n",
       "      <td>5.826099</td>\n",
       "      <td>3.203630</td>\n",
       "      <td>3.595112</td>\n",
       "      <td>281.975073</td>\n",
       "      <td>1.606329</td>\n",
       "      <td>1.373321</td>\n",
       "      <td>1.597086</td>\n",
       "      <td>3.706014e-06</td>\n",
       "      <td>23.082500</td>\n",
       "      <td>22.402090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>995.000000</td>\n",
       "      <td>5168.290000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>434.165000</td>\n",
       "      <td>523.852000</td>\n",
       "      <td>1120.785827</td>\n",
       "      <td>0.494396</td>\n",
       "      <td>106.084131</td>\n",
       "      <td>29.742400</td>\n",
       "      <td>...</td>\n",
       "      <td>18.019141</td>\n",
       "      <td>12.254442</td>\n",
       "      <td>13.361204</td>\n",
       "      <td>1755.800206</td>\n",
       "      <td>51.169090</td>\n",
       "      <td>47.244050</td>\n",
       "      <td>53.566903</td>\n",
       "      <td>1.670399e-02</td>\n",
       "      <td>5200.000000</td>\n",
       "      <td>547.103092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                V            M           n          np           B  \\\n",
       "count  370.000000   370.000000  370.000000  370.000000  370.000000   \n",
       "mean   145.503405   576.941738    6.810811    3.789189  111.257870   \n",
       "std    132.334810   625.505406    4.818313    3.150066   82.433063   \n",
       "min     11.410000     8.010000    2.000000    2.000000    7.904080   \n",
       "25%     53.210000   157.831500    3.000000    3.000000   47.720850   \n",
       "50%     76.590000   287.557000    5.000000    3.000000   92.003700   \n",
       "75%    219.020000   907.187000   12.000000    3.000000  160.493000   \n",
       "max    995.000000  5168.290000   29.000000   29.000000  434.165000   \n",
       "\n",
       "                G            E           v           H          B'  ...  \\\n",
       "count  370.000000   370.000000  370.000000  370.000000  370.000000  ...   \n",
       "mean    59.678087   147.105572    0.296581   10.521358   -6.982807  ...   \n",
       "std     77.165080   170.936177    0.083493   14.299973   11.632497  ...   \n",
       "min      0.406580     1.215183    0.000656    0.006197 -104.589000  ...   \n",
       "25%     19.017575    49.639698    0.250683    3.580389   -8.717083  ...   \n",
       "50%     44.813700   114.970283    0.303591    7.326539   -2.906780  ...   \n",
       "75%     67.233975   175.339469    0.344524   11.691259   -1.296300  ...   \n",
       "max    523.852000  1120.785827    0.494396  106.084131   29.742400  ...   \n",
       "\n",
       "               vL          vS          va           Θe         γel  \\\n",
       "count  370.000000  370.000000  370.000000   370.000000  370.000000   \n",
       "mean     5.385817    2.928926    3.256445   263.253037    1.594866   \n",
       "std      2.784979    1.973751    2.152686   225.042124    2.868737   \n",
       "min      1.559351    0.331246    0.379108    25.916963   -7.029521   \n",
       "25%      3.836785    1.889592    2.115443   144.201287    1.107011   \n",
       "50%      4.836986    2.496888    2.782292   210.531759    1.379069   \n",
       "75%      5.826099    3.203630    3.595112   281.975073    1.606329   \n",
       "max     18.019141   12.254442   13.361204  1755.800206   51.169090   \n",
       "\n",
       "              γes          γe             A        y-exp    y-theory  \n",
       "count  370.000000  370.000000  3.700000e+02   370.000000  370.000000  \n",
       "mean     1.119608    1.995672  9.121067e-05    44.079546   24.255426  \n",
       "std      4.816583    3.987999  9.802296e-04   277.377084   49.038768  \n",
       "min    -65.475244    0.414990  1.459528e-07     0.086000    0.026157  \n",
       "25%      0.624775    0.921418  9.899596e-07     3.887500    4.432038  \n",
       "50%      1.027563    1.199428  1.864054e-06    11.075000   12.781238  \n",
       "75%      1.373321    1.597086  3.706014e-06    23.082500   22.402090  \n",
       "max     47.244050   53.566903  1.670399e-02  5200.000000  547.103092  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(370, 23)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name        0\n",
       "V           0\n",
       "M           0\n",
       "n           0\n",
       "np          0\n",
       "B           0\n",
       "G           0\n",
       "E           0\n",
       "v           0\n",
       "H           0\n",
       "B'          0\n",
       "G'          0\n",
       "ρ           0\n",
       "vL          0\n",
       "vS          0\n",
       "va          0\n",
       "Θe          0\n",
       "γel         0\n",
       "γes         0\n",
       "γe          0\n",
       "A           0\n",
       "y-exp       0\n",
       "y-theory    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(len(df.n.unique()))\n",
    "print(len(df.np.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadman/anaconda3/lib/python3.8/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "#Replacing \"n\"\n",
    "X = df.loc[:, ['n']]\n",
    "y = df['y-exp']\n",
    "OHEncoder = ce.OneHotEncoder(cols = ['n'])\n",
    "newCols = OHEncoder.fit_transform(X, y)\n",
    "df = df.drop(['n'], axis = 1)\n",
    "df = df.join(newCols)\n",
    "\n",
    "#Replacing \"np\"\n",
    "X = df.loc[:, ['np']]\n",
    "y = df['y-exp']\n",
    "OHEncoder = ce.OneHotEncoder(cols = ['np'])\n",
    "newCols = OHEncoder.fit_transform(X, y)\n",
    "df = df.drop(['np'], axis = 1)\n",
    "df = df.join(newCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['Name', 'V', 'M', 'B', 'G', 'E', 'v', 'H', 'B'', 'G'', 'ρ', 'vL', 'vS',\n",
       "        'va', 'Θe', 'γel', 'γes', 'γe', 'A', 'y-exp', 'y-theory', 'n_1', 'n_2',\n",
       "        'n_3', 'n_4', 'n_5', 'n_6', 'n_7', 'n_8', 'n_9', 'n_10', 'n_11', 'n_12',\n",
       "        'n_13', 'n_14', 'n_15', 'np_1', 'np_2', 'np_3', 'np_4', 'np_5', 'np_6',\n",
       "        'np_7', 'np_8', 'np_9', 'np_10', 'np_11', 'np_12', 'np_13', 'np_14',\n",
       "        'np_15'],\n",
       "       dtype='object'),\n",
       " (51,))"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns, df.columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.47103092e+02, 4.34409393e+00, 8.83552653e+00, 1.76556363e+01,\n",
       "       4.08157516e+01, 3.04336639e+01, 1.16520522e+01, 4.06207592e+01,\n",
       "       6.14393581e+01, 1.46431122e+02, 1.30123539e+02, 4.01184072e+01,\n",
       "       3.43295845e+02, 2.66374859e+01, 2.05159539e+01, 2.65594241e+02,\n",
       "       2.22351199e+02, 2.31233787e+02, 1.92571258e+01, 1.79763713e+02,\n",
       "       1.67790030e+02, 1.78481073e+02, 1.40004820e+02, 2.24286312e+01,\n",
       "       1.71740875e+02, 1.64527923e+02, 1.51845435e+02, 1.67923922e+02,\n",
       "       2.37071428e+01, 1.66205902e+02, 2.03897001e+02, 1.92243680e+02,\n",
       "       2.49756459e+01, 1.72200844e+01, 1.87350480e+01, 1.16504218e+01,\n",
       "       2.54374342e+01, 2.02139390e+01, 2.24774423e+01, 1.20080075e+02,\n",
       "       2.47966820e+01, 8.79382272e+01, 3.49949208e+01, 1.09468699e+01,\n",
       "       1.25784369e-01, 2.20510643e+01, 3.12421349e+00, 1.97465160e+01,\n",
       "       1.90527938e+01, 1.91718068e+00, 2.39450091e+01, 9.73866313e+00,\n",
       "       3.03845914e+01, 3.20414393e+01, 3.89541097e+01, 8.96149223e+00,\n",
       "       2.85669570e+01, 6.65112711e+00, 3.40508285e+01, 3.05842531e+01,\n",
       "       3.39689710e+01, 2.05507017e-01, 2.23224679e+01, 2.45587087e+01,\n",
       "       8.29762935e+00, 7.36260300e+00, 2.80103698e+01, 3.16154980e+01,\n",
       "       2.15320886e+01, 2.26420780e+01, 2.63177523e+01, 4.67571703e+01,\n",
       "       4.30773755e+01, 6.09533909e+00, 8.65046547e+00, 2.34004053e+00,\n",
       "       7.75149142e+01, 5.47665407e+00, 3.53242012e+01, 3.94416000e+01,\n",
       "       1.20021433e+01, 5.08914221e+01, 1.69083294e+01, 7.21352016e+01,\n",
       "       3.14119588e+01, 1.27273819e+01, 1.51709553e+01, 3.28223207e+00,\n",
       "       2.02674189e+01, 3.30495737e+01, 2.90791084e+01, 1.83373582e+01,\n",
       "       3.96377487e+01, 1.18991829e+01, 1.75072170e+01, 3.27093480e+01,\n",
       "       1.71017892e+01, 1.13501037e+01, 2.30483093e+01, 1.77691891e+01,\n",
       "       2.19234709e+01, 3.41787100e+01, 2.27959070e+01, 1.63564010e+01,\n",
       "       2.16242032e+01, 2.00398992e+01, 2.27976789e+01, 1.37785223e+01,\n",
       "       2.75007793e+01, 1.99154628e+01, 2.47836148e+01, 1.73359633e+01,\n",
       "       3.77560599e+01, 3.32603395e+01, 9.43594512e+00, 1.84773780e+00,\n",
       "       1.88979225e+01, 7.90670048e+00, 1.69760930e+01, 9.18546172e+00,\n",
       "       4.08150166e+00, 7.06583292e+00, 1.65520539e+01, 2.38350165e+01,\n",
       "       2.57434287e+01, 2.00693898e+01, 1.70128236e+01, 1.61522492e+01,\n",
       "       1.88784441e+01, 2.72757751e+01, 1.66588056e+01, 1.74104882e+01,\n",
       "       1.59328786e+00, 1.95905369e+01, 1.79561618e+01, 2.10138204e+01,\n",
       "       6.72197104e+00, 3.26447628e+00, 1.82650146e+01, 2.34998389e+01,\n",
       "       2.16200609e+01, 1.44831472e+01, 1.33251499e+01, 2.80280149e+01,\n",
       "       1.61728870e+01, 3.04090316e+01, 1.62718422e+01, 1.51427223e+01,\n",
       "       1.12671499e+01, 2.14837982e+01, 3.78711938e+00, 1.71447893e+01,\n",
       "       1.04541827e+01, 6.93137464e-01, 6.85182043e+00, 3.06271582e+01,\n",
       "       1.57261540e+01, 1.64410479e+01, 2.01587368e+01, 1.33061087e+01,\n",
       "       1.88206552e+01, 5.79313705e+00, 2.25597395e+01, 4.24975766e+00,\n",
       "       2.83164783e+01, 1.53083268e+01, 1.15711100e+01, 2.14553190e+01,\n",
       "       1.74982841e+01, 1.55779971e+01, 2.35028412e+01, 1.29251645e+01,\n",
       "       1.64479824e+01, 2.85773772e+01, 1.39327147e+01, 1.35916819e+01,\n",
       "       2.15628345e+01, 3.05988048e+00, 1.48873844e+01, 3.21075227e+01,\n",
       "       1.43648492e+01, 2.30096056e+01, 1.84231554e+01, 2.05799561e+01,\n",
       "       1.19368553e+01, 1.43817415e+01, 3.57024924e+01, 5.21122737e+00,\n",
       "       4.32578101e+00, 5.58205288e+00, 2.17260629e+01, 1.01060723e+01,\n",
       "       5.75620721e+01, 1.32101550e+01, 9.83807090e+00, 2.37039997e+01,\n",
       "       4.68488544e+00, 2.60621562e+01, 1.27955307e+01, 9.87549460e+00,\n",
       "       7.49914674e+00, 1.39262986e+01, 1.74999981e+01, 1.43782552e+01,\n",
       "       5.20508520e+00, 8.88288366e+01, 4.37138446e+00, 2.95175539e+01,\n",
       "       7.02043759e+00, 9.73786277e+00, 1.67708733e+01, 9.74149792e-01,\n",
       "       1.95772805e+00, 2.78070601e+01, 3.53020193e+00, 1.13174313e+00,\n",
       "       2.21542397e+01, 1.31589185e+01, 1.26880042e+01, 2.25377429e+01,\n",
       "       2.07157375e+01, 3.69932145e+00, 1.76982382e+01, 1.48879151e+01,\n",
       "       1.41986224e+01, 2.61569259e-02, 1.00518692e+01, 1.05390989e+01,\n",
       "       3.29004360e+00, 1.28611323e+01, 7.23314103e+00, 4.50115231e+00,\n",
       "       3.00425481e+01, 8.94813453e+00, 3.61395081e+00, 6.15020170e+00,\n",
       "       1.95089564e+00, 1.75907381e+01, 3.02375834e+00, 3.95161943e+00,\n",
       "       1.62754419e+00, 3.96588971e+01, 1.28553912e+01, 2.11576445e+01,\n",
       "       1.05819956e+01, 5.43561186e+00, 6.55970550e+00, 3.29203773e+00,\n",
       "       1.59976574e+00, 1.80076513e+01, 3.02465067e+01, 1.34754760e+01,\n",
       "       4.56557464e+00, 7.09127007e+00, 1.96484112e+01, 5.39837312e+00,\n",
       "       9.50048390e+00, 1.81240545e+00, 6.17561643e+00, 7.01861804e+00,\n",
       "       5.29068353e+00, 2.41394394e+01, 2.05044185e+01, 3.33663123e+00,\n",
       "       7.62600521e+00, 1.23968055e+01, 5.68709847e+00, 1.31685828e+01,\n",
       "       7.70719278e+00, 4.65233947e+00, 4.88642760e+00, 5.17864922e+00,\n",
       "       6.26743790e-01, 8.21621408e+00, 1.43762124e+00, 4.24121070e+01,\n",
       "       9.59804261e+00, 2.18554701e+00, 2.38326378e+00, 1.74247346e+01,\n",
       "       3.04777912e+00, 1.22762446e+01, 3.68372851e+01, 8.10915946e+00,\n",
       "       1.18216929e+01, 3.92364798e+00, 9.46611215e+00, 3.26456748e+00,\n",
       "       7.03684942e-01, 1.10425509e+01, 7.90091851e+00, 1.19473279e+00,\n",
       "       6.85958280e+00, 3.78439485e+00, 1.27669448e+01, 3.25673800e+00,\n",
       "       2.62369831e+00, 1.35099485e+00, 1.16857362e+01, 7.93100912e+00,\n",
       "       3.11997448e+00, 7.56870471e-01, 4.36058846e+00, 3.09555325e+00,\n",
       "       1.96140873e+00, 3.46864117e+00, 4.92492235e-01, 8.36625635e+00,\n",
       "       1.93207700e+00, 9.74037753e+00, 2.89308690e+00, 1.03206281e+01,\n",
       "       6.87593537e+00, 1.43715298e+00, 7.89477442e+00, 6.30302415e+00,\n",
       "       1.59047843e+00, 3.81511016e+00, 1.68936933e+00, 1.54770536e+00,\n",
       "       1.10451988e+01, 9.05841890e+00, 8.54414730e+00, 3.07084806e+00,\n",
       "       6.02769170e+00, 2.26279898e+00, 1.07287924e+01, 1.45271856e+01,\n",
       "       4.32299292e-01, 2.55593828e+00, 1.91698568e+00, 2.49120440e+00,\n",
       "       1.32372237e+00, 2.92061388e+00, 6.55310223e+00, 2.51040062e+00,\n",
       "       6.94059003e+00, 5.38312696e+00, 1.62809413e+00, 2.04605824e+00,\n",
       "       2.00516012e+00, 1.26884651e+01, 4.81842258e+00, 5.41489760e+00,\n",
       "       1.36174036e+01, 2.75354892e+00, 2.51516109e+00, 1.60706585e+01,\n",
       "       8.55302941e-01, 4.40900036e+00, 6.20943773e+00, 1.82305786e+00,\n",
       "       2.71002748e+00, 2.54574190e+00, 7.62803305e-01, 1.34200585e+01,\n",
       "       1.59646311e+00, 1.05519943e+01, 1.49621502e+00, 7.92304811e-01,\n",
       "       1.71585188e+00, 2.52008552e-01, 9.20056249e-01, 1.06439966e+00,\n",
       "       1.50476311e+00, 3.55214587e-01, 2.92956897e-01, 4.72718721e-01,\n",
       "       9.19026080e-01, 9.01755135e-01])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_theory = df['y-theory'].values\n",
    "df = df.drop(columns=['y-theory'])\n",
    "y_theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = train_test_split(df, test_size=0.25, random_state=42, shuffle=True)\n",
    "# test, val = train_test_split(test, test_size=0.4, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = train['y-exp'].values\n",
    "# excluded_columns = [\"Name\"]\n",
    "# X_train = train.drop(excluded_columns, axis=1)\n",
    "\n",
    "# y_test = test['y-exp'].values\n",
    "# excluded_columns = [\"Name\"]\n",
    "# X_test = test.drop(excluded_columns, axis=1)\n",
    "\n",
    "# y_val = test['y-exp'].values\n",
    "# excluded_columns = [\"Name\"]\n",
    "# X_val = test.drop(excluded_columns, axis=1)\n",
    "\n",
    "# X_train.shape[1], X_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape, y_test.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = RobustScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "# X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = torch.from_numpy(y_train)\n",
    "# y_test = torch.from_numpy(y_test)\n",
    "# y_val = torch.from_numpy(y_val)\n",
    "# #y_train = y_train.view(y_train.shape[0], 1)\n",
    "# y_test = y_test.view(y_test.shape[0], 1)\n",
    "# y_val = y_val.view(y_val.shape[0], 1)\n",
    "# y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = torch.from_numpy(X_train)\n",
    "# X_test = torch.from_numpy(X_test)\n",
    "# X_val = torch.from_numpy(X_val)\n",
    "# X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = Data.TensorDataset(X_train, y_train)\n",
    "# train_loader = Data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
    "# len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #X_train = Variable(X_train).to(device)\n",
    "# X_test = Variable(X_test).to(device)\n",
    "# X_val = Variable(X_val).to(device)\n",
    "# #y_train = Variable(y_train).to(device)\n",
    "# y_test = Variable(y_test).to(device)\n",
    "# y_val = Variable(y_val).to(device)\n",
    "# X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IRNet, self).__init__()\n",
    "        \n",
    "        self.linearXto1024 = nn.Sequential(\n",
    "            nn.Linear(49, 1024, bias=True),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear1024to1024 = nn.Sequential(\n",
    "            nn.Linear(1024, 1024, bias=True),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear1024to512 = nn.Sequential(\n",
    "            nn.Linear(1024, 512, bias=True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear512to512 = nn.Sequential(\n",
    "            nn.Linear(512, 512, bias=True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear512to256 = nn.Sequential(\n",
    "            nn.Linear(512, 256, bias=True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear256to256 = nn.Sequential(\n",
    "            nn.Linear(256, 256, bias=True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear256to128 = nn.Sequential(\n",
    "            nn.Linear(256, 128, bias=True),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear128to128 = nn.Sequential(\n",
    "            nn.Linear(128, 128, bias=True),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear128to64 = nn.Sequential(\n",
    "            nn.Linear(128, 64, bias=True),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear64to64 = nn.Sequential(\n",
    "            nn.Linear(64, 64, bias=True),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear64to32 = nn.Sequential(\n",
    "            nn.Linear(64, 32, bias=True),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear32to32 = nn.Sequential(\n",
    "            nn.Linear(32, 32, bias=True),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear32to1 = nn.Linear(32, 1, bias=True)\n",
    "        \n",
    "        self.projectionXto1024 = nn.Linear(49, 1024, bias=True)\n",
    "        self.projection1024to512 = nn.Linear(1024, 512, bias=True)\n",
    "        self.projection512to256 = nn.Linear(512, 256, bias=True)\n",
    "        self.projection256to128 = nn.Linear(256, 128, bias=True)\n",
    "        self.projection128to64 = nn.Linear(128, 64, bias=True)\n",
    "        self.projection64to32 = nn.Linear(64, 32, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outPrev = x\n",
    "        \n",
    "        out = self.linearXto1024(x)\n",
    "        out = torch.add(out, self.projectionXto1024(outPrev))\n",
    "        outPrev = out\n",
    "        \n",
    "        out = self.linear1024to1024(out)\n",
    "        out = torch.add(out, outPrev)\n",
    "        outPrev = out\n",
    "        out = self.linear1024to1024(out)\n",
    "        out = torch.add(out, outPrev)\n",
    "        outPrev = out\n",
    "        out = self.linear1024to1024(out)\n",
    "        out = torch.add(out, outPrev)\n",
    "        outPrev = out\n",
    "        \n",
    "        out = self.linear1024to512(out)\n",
    "        out = torch.add(out, self.projection1024to512(outPrev))\n",
    "        outPrev = out\n",
    "        \n",
    "        out = self.linear512to512(out)\n",
    "        out = torch.add(out, outPrev)\n",
    "        outPrev = out\n",
    "        out = self.linear512to512(out)\n",
    "        out = torch.add(out, outPrev)\n",
    "        outPrev = out\n",
    "        \n",
    "        out = self.linear512to256(out)\n",
    "        out = torch.add(out, self.projection512to256(outPrev))\n",
    "        outPrev = out\n",
    "        \n",
    "        out = self.linear256to256(out)\n",
    "        out = torch.add(out, outPrev)\n",
    "        outPrev = out\n",
    "        out = self.linear256to256(out)\n",
    "        out = torch.add(out, outPrev)\n",
    "        outPrev = out\n",
    "        \n",
    "        out = self.linear256to128(out)\n",
    "        out = torch.add(out, self.projection256to128(outPrev))\n",
    "        outPrev = out\n",
    "        \n",
    "        out = self.linear128to128(out)\n",
    "        out = torch.add(out, outPrev)\n",
    "        outPrev = out\n",
    "        out = self.linear128to128(out)\n",
    "        out = torch.add(out, outPrev)\n",
    "        outPrev = out\n",
    "        \n",
    "        out = self.linear128to64(out)\n",
    "        out = torch.add(out, self.projection128to64(outPrev))\n",
    "        outPrev = out\n",
    "        \n",
    "        out = self.linear64to64(out)\n",
    "        out = torch.add(out, outPrev)\n",
    "        outPrev = out\n",
    "        \n",
    "        out = self.linear64to32(out)\n",
    "        out = torch.add(out, self.projection64to32(outPrev))\n",
    "        \n",
    "        out = self.linear32to1(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "#irnet = IRNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in irnet.state_dict().items():\n",
    "#     print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "learning_rate = 0.001\n",
    "#criterion = nn.L1Loss()\n",
    "criterion = nn.MSELoss()\n",
    "#optimizer = torch.optim.Adam(irnet.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses_train = []\n",
    "# losses_val = []\n",
    "# epochs = []\n",
    "# minLoss = 100.0\n",
    "# minOutput = torch.Tensor(X_train.shape[0], 1)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = irnet(X_train.float())\n",
    "#     val_outputs = irnet(X_val.float())\n",
    "#     #print(type(outputs), outputs.shape)\n",
    "#     #print(outputs)\n",
    "#     #print(final_output)\n",
    "#     #print(outputs.shape)\n",
    "#     #print(ytor.shape)\n",
    "#     loss = criterion(outputs, y_train.float())\n",
    "#     if loss.data < minLoss:\n",
    "#         minLoss = loss.data\n",
    "#         minOutput = outputs\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     print('Epoch [%d/%d], Loss: %.4f'\n",
    "#          %(epoch+1, num_epochs, loss.data))\n",
    "#     epochs.append(epoch)\n",
    "#     losses_train.append(loss)\n",
    "#     losses_val.append(criterion(val_outputs, y_val.float()))\n",
    "# #print(minLoss.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(losses_train)):\n",
    "#     losses_train[i] = losses_train[i].cpu().detach().numpy()\n",
    "#     losses_val[i] = losses_val[i].cpu().detach().numpy()\n",
    "# #     losses_train[i] = losses_train[i]\n",
    "# #     losses_val[i] = losses_val[i]\n",
    "# plt.plot(epochs, losses_train)\n",
    "# plt.plot(epochs, losses_val)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_output_train = irnet(X_train.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Results on training set using 17 layer IRNet after %d iterations:\\n------------------------------'%(num_epochs))\n",
    "# print('r2 score: ', r2_score(y_train.cpu().detach().numpy(), final_output_train.cpu().detach().numpy()))\n",
    "# print('Mean Absolute Error:', criterion(final_output_train, y_train.float()).cpu().detach().numpy())\n",
    "# print('Mean Squared Error: ', mean_squared_error(y_train.cpu().detach().numpy(), final_output_train.cpu().detach().numpy()))\n",
    "# print('Root mean squared error: ', np.sqrt(mean_squared_error(y_train.cpu().detach().numpy(), final_output_train.cpu().detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_output_test = irnet(X_test.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Results on test set using 17 layer IRNet after %d iterations:\\n------------------------------'%(num_epochs))\n",
    "# print('Mean Absolute Error:', criterion(final_output_test, y_test.float()).cpu().detach().numpy())\n",
    "# print('r2 score: ', r2_score(y_test.cpu().detach().numpy(), final_output_test.cpu().detach().numpy()))\n",
    "# print('Mean Squared Error: ', mean_squared_error(y_test.cpu().detach().numpy(), final_output_test.cpu().detach().numpy()))\n",
    "# print('Root mean squared error: ', np.sqrt(mean_squared_error(y_test.cpu().detach().numpy(), final_output_test.cpu().detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-1:\n",
      "----------------\n",
      "Epoch [1/300], Train loss: 2309.8411, Validation loss: 8104.2109\n",
      "Epoch [2/300], Train loss: 970.8712, Validation loss: 650.1130\n",
      "Epoch [3/300], Train loss: 4762.8071, Validation loss: 8063.4609\n",
      "Epoch [4/300], Train loss: 614.3819, Validation loss: 12687.9033\n",
      "Epoch [5/300], Train loss: 12906.9805, Validation loss: 13819.0811\n",
      "Epoch [6/300], Train loss: 2273.4360, Validation loss: 12828.2305\n",
      "Epoch [7/300], Train loss: 11379.9082, Validation loss: 11880.7207\n",
      "Epoch [8/300], Train loss: 4634.9146, Validation loss: 11514.0889\n",
      "Epoch [9/300], Train loss: 3016.0225, Validation loss: 11081.1260\n",
      "Epoch [10/300], Train loss: 1188.3755, Validation loss: 10245.0986\n",
      "Epoch [11/300], Train loss: 364.0681, Validation loss: 8853.2832\n",
      "Epoch [12/300], Train loss: 394.3803, Validation loss: 6102.2002\n",
      "Epoch [13/300], Train loss: 1274.9895, Validation loss: 1621.0010\n",
      "Epoch [14/300], Train loss: 8878.3037, Validation loss: 6971.1152\n",
      "Epoch [15/300], Train loss: 81649.2344, Validation loss: 4383.0127\n",
      "Epoch [16/300], Train loss: 988645.0625, Validation loss: 8603.7715\n",
      "Epoch [17/300], Train loss: 24402.4453, Validation loss: 12552.4248\n",
      "Epoch [18/300], Train loss: 655.0457, Validation loss: 13526.6592\n",
      "Epoch [19/300], Train loss: 10841.1992, Validation loss: 13187.4102\n",
      "Epoch [20/300], Train loss: 1253710.3750, Validation loss: 12949.8477\n",
      "Epoch [21/300], Train loss: 1318.0795, Validation loss: 12631.2822\n",
      "Epoch [22/300], Train loss: 2479.5415, Validation loss: 12867.2412\n",
      "Epoch [23/300], Train loss: 1240930.0000, Validation loss: 12700.7314\n",
      "Epoch [24/300], Train loss: 1225359.3750, Validation loss: 12359.3516\n",
      "Epoch [25/300], Train loss: 1003.7236, Validation loss: 11551.3691\n",
      "Epoch [26/300], Train loss: 7716.0527, Validation loss: 11552.0029\n",
      "Epoch [27/300], Train loss: 4882.3408, Validation loss: 10533.6572\n",
      "Epoch [28/300], Train loss: 523.2384, Validation loss: 9959.7559\n",
      "Epoch [29/300], Train loss: 3471.6592, Validation loss: 8623.7246\n",
      "Epoch [30/300], Train loss: 6917.5425, Validation loss: 6517.8564\n",
      "Epoch [31/300], Train loss: 227.6332, Validation loss: 6124.7632\n",
      "Epoch [32/300], Train loss: 230.7577, Validation loss: 6154.2148\n",
      "Epoch [33/300], Train loss: 43134.4180, Validation loss: 7036.5767\n",
      "Epoch [34/300], Train loss: 110.9202, Validation loss: 6719.2017\n",
      "Epoch [35/300], Train loss: 233.0534, Validation loss: 4734.4175\n",
      "Epoch [36/300], Train loss: 1003.6356, Validation loss: 3797.5461\n",
      "Epoch [37/300], Train loss: 794.7827, Validation loss: 1497.6571\n",
      "Epoch [38/300], Train loss: 5052.2153, Validation loss: 2922.3782\n",
      "Epoch [39/300], Train loss: 53297.8438, Validation loss: 4668.9961\n",
      "Epoch [40/300], Train loss: 102541.7031, Validation loss: 3330.9656\n",
      "Epoch [41/300], Train loss: 136451.0781, Validation loss: 303.2372\n",
      "Epoch [42/300], Train loss: 1035.8645, Validation loss: 441.7722\n",
      "Epoch [43/300], Train loss: 2252001.0000, Validation loss: 335.5321\n",
      "Epoch [44/300], Train loss: 5895.6743, Validation loss: 15282.9453\n",
      "Epoch [45/300], Train loss: 400.9196, Validation loss: 14616.1240\n",
      "Epoch [46/300], Train loss: 10281.8545, Validation loss: 11971.0596\n",
      "Epoch [47/300], Train loss: 713.0740, Validation loss: 9585.1104\n",
      "Epoch [48/300], Train loss: 1046544.6875, Validation loss: 9174.0400\n",
      "Epoch [49/300], Train loss: 21921.8418, Validation loss: 5389.0444\n",
      "Epoch [50/300], Train loss: 82.7513, Validation loss: 1307.9117\n",
      "Epoch [51/300], Train loss: 1076.5225, Validation loss: 499.3405\n",
      "Epoch [52/300], Train loss: 6516.8867, Validation loss: 4386.1709\n",
      "Epoch [53/300], Train loss: 2110.5261, Validation loss: 3623.8525\n",
      "Epoch [54/300], Train loss: 1301685.2500, Validation loss: 1671.9138\n",
      "Epoch [55/300], Train loss: 575.6163, Validation loss: 5058.2251\n",
      "Epoch [56/300], Train loss: 783.2725, Validation loss: 8412.1533\n",
      "Epoch [57/300], Train loss: 6132.0312, Validation loss: 9160.1152\n",
      "Epoch [58/300], Train loss: 2292.0190, Validation loss: 8155.7734\n",
      "Epoch [59/300], Train loss: 505.0021, Validation loss: 6059.9268\n",
      "Epoch [60/300], Train loss: 58.3374, Validation loss: 1958.0132\n",
      "Epoch [61/300], Train loss: 152.9793, Validation loss: 193.0044\n",
      "Epoch [62/300], Train loss: 908.1852, Validation loss: 1261.5251\n",
      "Epoch [63/300], Train loss: 3536.8286, Validation loss: 4203.6665\n",
      "Epoch [64/300], Train loss: 721.3028, Validation loss: 1933.4679\n",
      "Epoch [65/300], Train loss: 1769.2596, Validation loss: 709.5428\n",
      "Epoch [66/300], Train loss: 8469.9316, Validation loss: 508.0650\n",
      "Epoch [67/300], Train loss: 522.3629, Validation loss: 371.5511\n",
      "Epoch [68/300], Train loss: 1250.7156, Validation loss: 734.4672\n",
      "Epoch [69/300], Train loss: 286.4064, Validation loss: 817.7814\n",
      "Epoch [70/300], Train loss: 348.7283, Validation loss: 389.7650\n",
      "Epoch [71/300], Train loss: 1117.3650, Validation loss: 277.2776\n",
      "Epoch [72/300], Train loss: 750.0897, Validation loss: 376.3098\n",
      "Epoch [73/300], Train loss: 132.2163, Validation loss: 369.0123\n",
      "Epoch [74/300], Train loss: 462.0079, Validation loss: 524.2836\n",
      "Epoch [75/300], Train loss: 450.8399, Validation loss: 395.2620\n",
      "Epoch [76/300], Train loss: 348.2776, Validation loss: 348.8839\n",
      "Epoch [77/300], Train loss: 310.7443, Validation loss: 197.4951\n",
      "Epoch [78/300], Train loss: 58.7000, Validation loss: 208.9060\n",
      "Epoch [79/300], Train loss: 219.4740, Validation loss: 340.9677\n",
      "Epoch [80/300], Train loss: 262.0938, Validation loss: 269.8628\n",
      "Epoch [81/300], Train loss: 156.4542, Validation loss: 184.8602\n",
      "Epoch [82/300], Train loss: 131.4057, Validation loss: 176.2858\n",
      "Epoch [83/300], Train loss: 124.5718, Validation loss: 210.9081\n",
      "Epoch [84/300], Train loss: 1194.5956, Validation loss: 231.9328\n",
      "Epoch [85/300], Train loss: 901.7618, Validation loss: 279.0197\n",
      "Epoch [86/300], Train loss: 96.3170, Validation loss: 188.7695\n",
      "Epoch [87/300], Train loss: 36.8341, Validation loss: 175.1069\n",
      "Epoch [88/300], Train loss: 301.5453, Validation loss: 196.1653\n",
      "Epoch [89/300], Train loss: 279.8376, Validation loss: 321.9883\n",
      "Epoch [90/300], Train loss: 40.9668, Validation loss: 384.2463\n",
      "Epoch [91/300], Train loss: 82.2144, Validation loss: 236.9089\n",
      "Epoch [92/300], Train loss: 258.1541, Validation loss: 261.8130\n",
      "Epoch [93/300], Train loss: 231.4071, Validation loss: 231.7583\n",
      "Epoch [94/300], Train loss: 1953.5839, Validation loss: 171.7558\n",
      "Epoch [95/300], Train loss: 106.9657, Validation loss: 224.3641\n",
      "Epoch [96/300], Train loss: 84.2711, Validation loss: 183.4788\n",
      "Epoch [97/300], Train loss: 51.6362, Validation loss: 152.1522\n",
      "Epoch [98/300], Train loss: 1534.4127, Validation loss: 128.9357\n",
      "Epoch [99/300], Train loss: 66.7094, Validation loss: 190.9415\n",
      "Epoch [100/300], Train loss: 156.3139, Validation loss: 151.5405\n",
      "Epoch [101/300], Train loss: 413.1396, Validation loss: 131.1612\n",
      "Epoch [102/300], Train loss: 50.5688, Validation loss: 147.3884\n",
      "Epoch [103/300], Train loss: 303.6132, Validation loss: 225.9757\n",
      "Epoch [104/300], Train loss: 1844.4260, Validation loss: 138.7585\n",
      "Epoch [105/300], Train loss: 7608.3179, Validation loss: 123.7160\n",
      "Epoch [106/300], Train loss: 68.3334, Validation loss: 127.4117\n",
      "Epoch [107/300], Train loss: 42.9805, Validation loss: 254.3529\n",
      "Epoch [108/300], Train loss: 721.7181, Validation loss: 167.7322\n",
      "Epoch [109/300], Train loss: 135.8231, Validation loss: 130.7656\n",
      "Epoch [110/300], Train loss: 761.2985, Validation loss: 204.9773\n",
      "Epoch [111/300], Train loss: 720.6262, Validation loss: 266.0227\n",
      "Epoch [112/300], Train loss: 59444.0000, Validation loss: 170.8298\n",
      "Epoch [113/300], Train loss: 98.6903, Validation loss: 167.9152\n",
      "Epoch [114/300], Train loss: 73.0919, Validation loss: 320.4566\n",
      "Epoch [115/300], Train loss: 85.6971, Validation loss: 355.6628\n",
      "Epoch [116/300], Train loss: 31512.4336, Validation loss: 326.7899\n",
      "Epoch [117/300], Train loss: 51.6286, Validation loss: 228.4240\n",
      "Epoch [118/300], Train loss: 65.9964, Validation loss: 690.8703\n",
      "Epoch [119/300], Train loss: 95.0944, Validation loss: 473.0039\n",
      "Epoch [120/300], Train loss: 112.9557, Validation loss: 281.0571\n",
      "Epoch [121/300], Train loss: 60.9086, Validation loss: 201.6911\n",
      "Epoch [122/300], Train loss: 409.2729, Validation loss: 240.0080\n",
      "Epoch [123/300], Train loss: 24.9359, Validation loss: 314.6427\n",
      "Epoch [124/300], Train loss: 15.5960, Validation loss: 241.6435\n",
      "Epoch [125/300], Train loss: 45.1346, Validation loss: 207.8262\n",
      "Epoch [126/300], Train loss: 265.8904, Validation loss: 205.0008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [127/300], Train loss: 91.5433, Validation loss: 259.6175\n",
      "Epoch [128/300], Train loss: 45.3403, Validation loss: 557.0303\n",
      "Epoch [129/300], Train loss: 27.1581, Validation loss: 304.2428\n",
      "Epoch [130/300], Train loss: 300.5128, Validation loss: 183.9917\n",
      "Epoch [131/300], Train loss: 124.9071, Validation loss: 149.5026\n",
      "Epoch [132/300], Train loss: 97.9590, Validation loss: 240.6421\n",
      "Epoch [133/300], Train loss: 2320.3003, Validation loss: 390.3127\n",
      "Epoch [134/300], Train loss: 27.9599, Validation loss: 170.8276\n",
      "Epoch [135/300], Train loss: 154.5733, Validation loss: 239.9245\n",
      "Epoch [136/300], Train loss: 102.4067, Validation loss: 401.7936\n",
      "Epoch [137/300], Train loss: 161.9100, Validation loss: 244.0932\n",
      "Epoch [138/300], Train loss: 16.5138, Validation loss: 191.7518\n",
      "Epoch [139/300], Train loss: 626.8087, Validation loss: 164.8553\n",
      "Epoch [140/300], Train loss: 114.1499, Validation loss: 283.4622\n",
      "Epoch [141/300], Train loss: 2470.1902, Validation loss: 369.3896\n",
      "Epoch [142/300], Train loss: 3638.5278, Validation loss: 160.3202\n",
      "Epoch [143/300], Train loss: 17.8692, Validation loss: 129.4500\n",
      "Epoch [144/300], Train loss: 115.0118, Validation loss: 648.7123\n",
      "Epoch [145/300], Train loss: 20.2982, Validation loss: 243.0199\n",
      "Epoch [146/300], Train loss: 62.2853, Validation loss: 249.1597\n",
      "Epoch [147/300], Train loss: 19.1367, Validation loss: 257.7184\n",
      "Epoch [148/300], Train loss: 31.3103, Validation loss: 245.5765\n",
      "Epoch [149/300], Train loss: 34.4915, Validation loss: 300.8163\n",
      "Epoch [150/300], Train loss: 10.0967, Validation loss: 254.3963\n",
      "Epoch [151/300], Train loss: 12.4187, Validation loss: 225.4363\n",
      "Epoch [152/300], Train loss: 854.5483, Validation loss: 208.7089\n",
      "Epoch [153/300], Train loss: 720.4663, Validation loss: 183.1536\n",
      "Epoch [154/300], Train loss: 42.7118, Validation loss: 240.3581\n",
      "Epoch [155/300], Train loss: 47.9796, Validation loss: 201.4582\n",
      "Epoch [156/300], Train loss: 291.4477, Validation loss: 235.7937\n",
      "Epoch [157/300], Train loss: 47.6364, Validation loss: 246.5557\n",
      "Epoch [158/300], Train loss: 75.9849, Validation loss: 181.6069\n",
      "Epoch [159/300], Train loss: 65.1540, Validation loss: 252.2223\n",
      "Epoch [160/300], Train loss: 39.2187, Validation loss: 322.3485\n",
      "Epoch [161/300], Train loss: 2194.2576, Validation loss: 319.2437\n",
      "Epoch [162/300], Train loss: 97.0303, Validation loss: 176.7217\n",
      "Epoch [163/300], Train loss: 5.4771, Validation loss: 222.8509\n",
      "Epoch [164/300], Train loss: 14.6761, Validation loss: 339.0245\n",
      "Epoch [165/300], Train loss: 41.0762, Validation loss: 346.0029\n",
      "Epoch [166/300], Train loss: 30.1453, Validation loss: 291.2480\n",
      "Epoch [167/300], Train loss: 6.7557, Validation loss: 272.3415\n",
      "Epoch [168/300], Train loss: 178.4811, Validation loss: 247.1918\n",
      "Epoch [169/300], Train loss: 66.8795, Validation loss: 155.9452\n",
      "Epoch [170/300], Train loss: 27.7499, Validation loss: 166.8762\n",
      "Epoch [171/300], Train loss: 176.9283, Validation loss: 238.6555\n",
      "Epoch [172/300], Train loss: 8.4218, Validation loss: 193.5369\n",
      "Epoch [173/300], Train loss: 1505.3044, Validation loss: 155.8181\n",
      "Epoch [174/300], Train loss: 24.6040, Validation loss: 373.3896\n",
      "Epoch [175/300], Train loss: 41.9161, Validation loss: 206.9645\n",
      "Epoch [176/300], Train loss: 84.6337, Validation loss: 148.8506\n",
      "Epoch [177/300], Train loss: 8.4310, Validation loss: 180.8183\n",
      "Epoch [178/300], Train loss: 14.6612, Validation loss: 383.3078\n",
      "Epoch [179/300], Train loss: 68.0120, Validation loss: 138.9776\n",
      "Epoch [180/300], Train loss: 6.3982, Validation loss: 138.2122\n",
      "Epoch [181/300], Train loss: 12.3704, Validation loss: 231.2746\n",
      "Epoch [182/300], Train loss: 46.0921, Validation loss: 173.3355\n",
      "Epoch [183/300], Train loss: 12.1862, Validation loss: 126.7585\n",
      "Epoch [184/300], Train loss: 15.5902, Validation loss: 124.4533\n",
      "Epoch [185/300], Train loss: 55.7552, Validation loss: 261.4584\n",
      "Epoch [186/300], Train loss: 7.1009, Validation loss: 142.3610\n",
      "Epoch [187/300], Train loss: 22.1090, Validation loss: 136.6918\n",
      "Epoch [188/300], Train loss: 17.5190, Validation loss: 131.5147\n",
      "Epoch [189/300], Train loss: 7.2690, Validation loss: 121.1276\n",
      "Epoch [190/300], Train loss: 566.0139, Validation loss: 233.6293\n",
      "Epoch [191/300], Train loss: 16.5809, Validation loss: 128.8014\n",
      "Epoch [192/300], Train loss: 47.7505, Validation loss: 122.8703\n",
      "Epoch [193/300], Train loss: 69.5593, Validation loss: 206.4843\n",
      "Epoch [194/300], Train loss: 3169.6677, Validation loss: 121.9712\n",
      "Epoch [195/300], Train loss: 28.2940, Validation loss: 108.9016\n",
      "Epoch [196/300], Train loss: 14.4356, Validation loss: 111.4565\n",
      "Epoch [197/300], Train loss: 308.3036, Validation loss: 219.0590\n",
      "Epoch [198/300], Train loss: 25.7223, Validation loss: 155.2115\n",
      "Epoch [199/300], Train loss: 24.9569, Validation loss: 125.0686\n",
      "Epoch [200/300], Train loss: 24435.5215, Validation loss: 117.7243\n",
      "Epoch [201/300], Train loss: 69.5756, Validation loss: 84.1230\n",
      "Epoch [202/300], Train loss: 85.1929, Validation loss: 99.7795\n",
      "Epoch [203/300], Train loss: 155.4777, Validation loss: 1134.6903\n",
      "Epoch [204/300], Train loss: 729.7382, Validation loss: 1094.9047\n",
      "Epoch [205/300], Train loss: 57.8376, Validation loss: 427.3622\n",
      "Epoch [206/300], Train loss: 65.1292, Validation loss: 506.2256\n",
      "Epoch [207/300], Train loss: 204.1130, Validation loss: 1531.5889\n",
      "Epoch [208/300], Train loss: 794.6092, Validation loss: 738.5790\n",
      "Epoch [209/300], Train loss: 71.1843, Validation loss: 332.5659\n",
      "Epoch [210/300], Train loss: 52.4686, Validation loss: 1248.6995\n",
      "Epoch [211/300], Train loss: 155.9717, Validation loss: 830.5237\n",
      "Epoch [212/300], Train loss: 51.7049, Validation loss: 275.2816\n",
      "Epoch [213/300], Train loss: 90.4291, Validation loss: 1236.0078\n",
      "Epoch [214/300], Train loss: 274.0586, Validation loss: 1186.8434\n",
      "Epoch [215/300], Train loss: 34.2407, Validation loss: 247.4998\n",
      "Epoch [216/300], Train loss: 211.2130, Validation loss: 226.6310\n",
      "Epoch [217/300], Train loss: 64.1908, Validation loss: 595.3861\n",
      "Epoch [218/300], Train loss: 23.8905, Validation loss: 440.1024\n",
      "Epoch [219/300], Train loss: 50.0204, Validation loss: 194.5064\n",
      "Epoch [220/300], Train loss: 62.3235, Validation loss: 433.4486\n",
      "Epoch [221/300], Train loss: 21.7160, Validation loss: 585.8920\n",
      "Epoch [222/300], Train loss: 32.5290, Validation loss: 175.0490\n",
      "Epoch [223/300], Train loss: 47.4900, Validation loss: 281.6465\n",
      "Epoch [224/300], Train loss: 207.6915, Validation loss: 1138.3712\n",
      "Epoch [225/300], Train loss: 4.0086, Validation loss: 259.1222\n",
      "Epoch [226/300], Train loss: 38.2286, Validation loss: 162.3967\n",
      "Epoch [227/300], Train loss: 64.8457, Validation loss: 282.2613\n",
      "Epoch [228/300], Train loss: 100.6490, Validation loss: 182.2225\n",
      "Epoch [229/300], Train loss: 40.8227, Validation loss: 332.5589\n",
      "Epoch [230/300], Train loss: 124.2972, Validation loss: 263.6097\n",
      "Epoch [231/300], Train loss: 37.6670, Validation loss: 133.5311\n",
      "Epoch [232/300], Train loss: 29.8140, Validation loss: 118.5928\n",
      "Epoch [233/300], Train loss: 11013.4111, Validation loss: 892.9706\n",
      "Epoch [234/300], Train loss: 31.0632, Validation loss: 158.5308\n",
      "Epoch [235/300], Train loss: 1112.0414, Validation loss: 685.5726\n",
      "Epoch [236/300], Train loss: 739.2916, Validation loss: 526.3658\n",
      "Epoch [237/300], Train loss: 5.9753, Validation loss: 596.5620\n",
      "Epoch [238/300], Train loss: 40.3976, Validation loss: 488.4664\n",
      "Epoch [239/300], Train loss: 17.6734, Validation loss: 632.7733\n",
      "Epoch [240/300], Train loss: 134.8287, Validation loss: 306.7391\n",
      "Epoch [241/300], Train loss: 49.0728, Validation loss: 531.7529\n",
      "Epoch [242/300], Train loss: 1699.8818, Validation loss: 656.8885\n",
      "Epoch [243/300], Train loss: 483.9737, Validation loss: 206.8080\n",
      "Epoch [244/300], Train loss: 76.5061, Validation loss: 180.3840\n",
      "Epoch [245/300], Train loss: 137.5484, Validation loss: 4143.6382\n",
      "Epoch [246/300], Train loss: 222.9472, Validation loss: 1223.8466\n",
      "Epoch [247/300], Train loss: 269.5144, Validation loss: 764.6275\n",
      "Epoch [248/300], Train loss: 23951.7656, Validation loss: 111.5345\n",
      "Epoch [249/300], Train loss: 91.2901, Validation loss: 708.6223\n",
      "Epoch [250/300], Train loss: 31.8111, Validation loss: 89.9928\n",
      "Epoch [251/300], Train loss: 175.9546, Validation loss: 84.1000\n",
      "Epoch [252/300], Train loss: 31.0834, Validation loss: 326.8210\n",
      "Epoch [253/300], Train loss: 125.6400, Validation loss: 401.1846\n",
      "Epoch [254/300], Train loss: 49.6261, Validation loss: 148.1007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [255/300], Train loss: 32.3453, Validation loss: 124.0834\n",
      "Epoch [256/300], Train loss: 72.6008, Validation loss: 780.5040\n",
      "Epoch [257/300], Train loss: 53.9119, Validation loss: 236.2103\n",
      "Epoch [258/300], Train loss: 53.5920, Validation loss: 153.3633\n",
      "Epoch [259/300], Train loss: 15412.7676, Validation loss: 136.0977\n",
      "Epoch [260/300], Train loss: 115.2659, Validation loss: 1432.8756\n",
      "Epoch [261/300], Train loss: 22.1547, Validation loss: 177.4433\n",
      "Epoch [262/300], Train loss: 95.2331, Validation loss: 261.2339\n",
      "Epoch [263/300], Train loss: 91.8228, Validation loss: 783.5559\n",
      "Epoch [264/300], Train loss: 41.0724, Validation loss: 558.7222\n",
      "Epoch [265/300], Train loss: 4671.0298, Validation loss: 206.6319\n",
      "Epoch [266/300], Train loss: 100.7942, Validation loss: 668.3789\n",
      "Epoch [267/300], Train loss: 53.6434, Validation loss: 359.7786\n",
      "Epoch [268/300], Train loss: 47.1360, Validation loss: 195.7070\n",
      "Epoch [269/300], Train loss: 27.7066, Validation loss: 351.5658\n",
      "Epoch [270/300], Train loss: 16.2019, Validation loss: 245.7504\n",
      "Epoch [271/300], Train loss: 2949.0825, Validation loss: 241.9016\n",
      "Epoch [272/300], Train loss: 56.0061, Validation loss: 665.6736\n",
      "Epoch [273/300], Train loss: 27.4814, Validation loss: 181.5002\n",
      "Epoch [274/300], Train loss: 25.0688, Validation loss: 191.2771\n",
      "Epoch [275/300], Train loss: 40.4885, Validation loss: 694.1788\n",
      "Epoch [276/300], Train loss: 8.2403, Validation loss: 421.1396\n",
      "Epoch [277/300], Train loss: 28.1991, Validation loss: 245.6640\n",
      "Epoch [278/300], Train loss: 25.0724, Validation loss: 339.8781\n",
      "Epoch [279/300], Train loss: 9.7207, Validation loss: 632.6636\n",
      "Epoch [280/300], Train loss: 8125.5054, Validation loss: 835.7496\n",
      "Epoch [281/300], Train loss: 75.5038, Validation loss: 197.4806\n",
      "Epoch [282/300], Train loss: 7.9086, Validation loss: 371.3303\n",
      "Epoch [283/300], Train loss: 4334.9966, Validation loss: 777.0856\n",
      "Epoch [284/300], Train loss: 116.8593, Validation loss: 185.6349\n",
      "Epoch [285/300], Train loss: 166.8023, Validation loss: 490.2176\n",
      "Epoch [286/300], Train loss: 17.6518, Validation loss: 713.0251\n",
      "Epoch [287/300], Train loss: 10.2245, Validation loss: 327.4131\n",
      "Epoch [288/300], Train loss: 542.2224, Validation loss: 289.8147\n",
      "Epoch [289/300], Train loss: 4.0478, Validation loss: 355.5804\n",
      "Epoch [290/300], Train loss: 28.8007, Validation loss: 486.4666\n",
      "Epoch [291/300], Train loss: 3309.6245, Validation loss: 577.0786\n",
      "Epoch [292/300], Train loss: 405.6258, Validation loss: 196.7062\n",
      "Epoch [293/300], Train loss: 1706.7958, Validation loss: 242.2854\n",
      "Epoch [294/300], Train loss: 8.4187, Validation loss: 566.8057\n",
      "Epoch [295/300], Train loss: 15.4466, Validation loss: 281.4735\n",
      "Epoch [296/300], Train loss: 185.6819, Validation loss: 146.2924\n",
      "Epoch [297/300], Train loss: 42.8246, Validation loss: 288.7970\n",
      "Epoch [298/300], Train loss: 19.3397, Validation loss: 271.3786\n",
      "Epoch [299/300], Train loss: 13.3078, Validation loss: 446.2839\n",
      "Epoch [300/300], Train loss: 13.7681, Validation loss: 192.5216\n",
      "lowest validation error achieved : 84.1000\n",
      "test error:  79.26834942766163\n",
      "\n",
      "\n",
      "CV-2:\n",
      "----------------\n",
      "Epoch [1/300], Train loss: 3201.6370, Validation loss: 2626.3779\n",
      "Epoch [2/300], Train loss: 1453.8010, Validation loss: 2557.8691\n",
      "Epoch [3/300], Train loss: 156.4622, Validation loss: 2788.0032\n",
      "Epoch [4/300], Train loss: 8677.5137, Validation loss: 2391.8142\n",
      "Epoch [5/300], Train loss: 10886.1514, Validation loss: 763.1004\n",
      "Epoch [6/300], Train loss: 286.8323, Validation loss: 602.5790\n",
      "Epoch [7/300], Train loss: 137.7024, Validation loss: 1862.0588\n",
      "Epoch [8/300], Train loss: 749.8085, Validation loss: 3149.3865\n",
      "Epoch [9/300], Train loss: 1268457.3750, Validation loss: 3100.4038\n",
      "Epoch [10/300], Train loss: 11153.2637, Validation loss: 1200.1642\n",
      "Epoch [11/300], Train loss: 1012.0936, Validation loss: 1647.0935\n",
      "Epoch [12/300], Train loss: 19468.8516, Validation loss: 19571.1543\n",
      "Epoch [13/300], Train loss: 15676.0674, Validation loss: 8873.4727\n",
      "Epoch [14/300], Train loss: 520.0792, Validation loss: 2081.4451\n",
      "Epoch [15/300], Train loss: 10398.2705, Validation loss: 3452.7888\n",
      "Epoch [16/300], Train loss: 4330.5767, Validation loss: 2960.2625\n",
      "Epoch [17/300], Train loss: 1542.2227, Validation loss: 2329.3071\n",
      "Epoch [18/300], Train loss: 278.0745, Validation loss: 1944.4792\n",
      "Epoch [19/300], Train loss: 47987.1055, Validation loss: 1204.8828\n",
      "Epoch [20/300], Train loss: 528612.3125, Validation loss: 567.3849\n",
      "Epoch [21/300], Train loss: 124572.1328, Validation loss: 1613.8270\n",
      "Epoch [22/300], Train loss: 378.6114, Validation loss: 539.4561\n",
      "Epoch [23/300], Train loss: 8894.3828, Validation loss: 412.3332\n",
      "Epoch [24/300], Train loss: 37764.7422, Validation loss: 2583.7703\n",
      "Epoch [25/300], Train loss: 117921.5000, Validation loss: 2881.6030\n",
      "Epoch [26/300], Train loss: 7323.6562, Validation loss: 4527.8276\n",
      "Epoch [27/300], Train loss: 284.2917, Validation loss: 1509.7352\n",
      "Epoch [28/300], Train loss: 1488.0748, Validation loss: 1488.6656\n",
      "Epoch [29/300], Train loss: 1610.5288, Validation loss: 1073.2112\n",
      "Epoch [30/300], Train loss: 856.7056, Validation loss: 1419.4062\n",
      "Epoch [31/300], Train loss: 10111.3037, Validation loss: 3159.8374\n",
      "Epoch [32/300], Train loss: 303.5451, Validation loss: 1905.3195\n",
      "Epoch [33/300], Train loss: 790.0759, Validation loss: 1370.4973\n",
      "Epoch [34/300], Train loss: 550.7601, Validation loss: 264.8971\n",
      "Epoch [35/300], Train loss: 412.6053, Validation loss: 321.7733\n",
      "Epoch [36/300], Train loss: 123202.5000, Validation loss: 3320.2244\n",
      "Epoch [37/300], Train loss: 255995.4062, Validation loss: 951.2008\n",
      "Epoch [38/300], Train loss: 3745.0405, Validation loss: 1239.4480\n",
      "Epoch [39/300], Train loss: 23404.7090, Validation loss: 1871.8358\n",
      "Epoch [40/300], Train loss: 7553.3066, Validation loss: 3117.4775\n",
      "Epoch [41/300], Train loss: 1296.2803, Validation loss: 1027.8895\n",
      "Epoch [42/300], Train loss: 99.7814, Validation loss: 151.3349\n",
      "Epoch [43/300], Train loss: 1319.8689, Validation loss: 2637.5637\n",
      "Epoch [44/300], Train loss: 4615.9600, Validation loss: 1633.3739\n",
      "Epoch [45/300], Train loss: 4370.9849, Validation loss: 367.5419\n",
      "Epoch [46/300], Train loss: 40509.1055, Validation loss: 305.2076\n",
      "Epoch [47/300], Train loss: 2544.9980, Validation loss: 764.9981\n",
      "Epoch [48/300], Train loss: 1238.1735, Validation loss: 662.7343\n",
      "Epoch [49/300], Train loss: 108.6763, Validation loss: 344.5145\n",
      "Epoch [50/300], Train loss: 838.5944, Validation loss: 266.9521\n",
      "Epoch [51/300], Train loss: 1085.5050, Validation loss: 223.4852\n",
      "Epoch [52/300], Train loss: 479.4127, Validation loss: 184.3717\n",
      "Epoch [53/300], Train loss: 186.5592, Validation loss: 138.3674\n",
      "Epoch [54/300], Train loss: 141.5929, Validation loss: 134.5790\n",
      "Epoch [55/300], Train loss: 1074.4928, Validation loss: 116.9785\n",
      "Epoch [56/300], Train loss: 665.8302, Validation loss: 148.2474\n",
      "Epoch [57/300], Train loss: 51.3062, Validation loss: 84.0469\n",
      "Epoch [58/300], Train loss: 558.2347, Validation loss: 73.9664\n",
      "Epoch [59/300], Train loss: 362.0246, Validation loss: 62.0557\n",
      "Epoch [60/300], Train loss: 2533.4885, Validation loss: 97.1260\n",
      "Epoch [61/300], Train loss: 177.0550, Validation loss: 34.4353\n",
      "Epoch [62/300], Train loss: 509.6348, Validation loss: 36.7748\n",
      "Epoch [63/300], Train loss: 198.3662, Validation loss: 103.0930\n",
      "Epoch [64/300], Train loss: 210.8629, Validation loss: 52.1652\n",
      "Epoch [65/300], Train loss: 18829.4902, Validation loss: 77.4319\n",
      "Epoch [66/300], Train loss: 592.8997, Validation loss: 125.5084\n",
      "Epoch [67/300], Train loss: 40.7371, Validation loss: 69.6728\n",
      "Epoch [68/300], Train loss: 501.7917, Validation loss: 56.7058\n",
      "Epoch [69/300], Train loss: 438.7197, Validation loss: 86.5339\n",
      "Epoch [70/300], Train loss: 63.9206, Validation loss: 68.5247\n",
      "Epoch [71/300], Train loss: 635.8490, Validation loss: 39.2109\n",
      "Epoch [72/300], Train loss: 247.5613, Validation loss: 109.5631\n",
      "Epoch [73/300], Train loss: 150.4044, Validation loss: 56.3407\n",
      "Epoch [74/300], Train loss: 46.7673, Validation loss: 70.6914\n",
      "Epoch [75/300], Train loss: 145.1982, Validation loss: 68.5284\n",
      "Epoch [76/300], Train loss: 44.5183, Validation loss: 34.9550\n",
      "Epoch [77/300], Train loss: 91.6278, Validation loss: 75.5817\n",
      "Epoch [78/300], Train loss: 240.0929, Validation loss: 42.9551\n",
      "Epoch [79/300], Train loss: 73.7357, Validation loss: 41.9091\n",
      "Epoch [80/300], Train loss: 300.9283, Validation loss: 44.1487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [81/300], Train loss: 145.9429, Validation loss: 58.7848\n",
      "Epoch [82/300], Train loss: 106.3717, Validation loss: 32.7370\n",
      "Epoch [83/300], Train loss: 158.0344, Validation loss: 82.1772\n",
      "Epoch [84/300], Train loss: 7294.0640, Validation loss: 84.5913\n",
      "Epoch [85/300], Train loss: 1317.7729, Validation loss: 21.1719\n",
      "Epoch [86/300], Train loss: 228.2665, Validation loss: 32.3131\n",
      "Epoch [87/300], Train loss: 43949.3906, Validation loss: 59.2698\n",
      "Epoch [88/300], Train loss: 7317.5952, Validation loss: 58.4323\n",
      "Epoch [89/300], Train loss: 29.6535, Validation loss: 89.0562\n",
      "Epoch [90/300], Train loss: 110687.5391, Validation loss: 194.3358\n",
      "Epoch [91/300], Train loss: 7493.5967, Validation loss: 770.6823\n",
      "Epoch [92/300], Train loss: 125.9976, Validation loss: 187.9552\n",
      "Epoch [93/300], Train loss: 63531.5703, Validation loss: 219.9807\n",
      "Epoch [94/300], Train loss: 175.7048, Validation loss: 100.6188\n",
      "Epoch [95/300], Train loss: 63.6572, Validation loss: 61.3611\n",
      "Epoch [96/300], Train loss: 31.1173, Validation loss: 67.8593\n",
      "Epoch [97/300], Train loss: 659.9995, Validation loss: 215.3052\n",
      "Epoch [98/300], Train loss: 8.4397, Validation loss: 118.3647\n",
      "Epoch [99/300], Train loss: 240.9278, Validation loss: 53.9453\n",
      "Epoch [100/300], Train loss: 324.9749, Validation loss: 58.6009\n",
      "Epoch [101/300], Train loss: 149.8648, Validation loss: 42.9252\n",
      "Epoch [102/300], Train loss: 216.2297, Validation loss: 35.3498\n",
      "Epoch [103/300], Train loss: 117.4811, Validation loss: 27.2672\n",
      "Epoch [104/300], Train loss: 31.5851, Validation loss: 24.7351\n",
      "Epoch [105/300], Train loss: 341.4388, Validation loss: 23.3573\n",
      "Epoch [106/300], Train loss: 259.8015, Validation loss: 23.8082\n",
      "Epoch [107/300], Train loss: 58.4969, Validation loss: 107.0393\n",
      "Epoch [108/300], Train loss: 91.1913, Validation loss: 51.9215\n",
      "Epoch [109/300], Train loss: 464.8078, Validation loss: 101.2233\n",
      "Epoch [110/300], Train loss: 43.6000, Validation loss: 40.7224\n",
      "Epoch [111/300], Train loss: 369.4564, Validation loss: 71.1747\n",
      "Epoch [112/300], Train loss: 78.3851, Validation loss: 36.2610\n",
      "Epoch [113/300], Train loss: 277.8470, Validation loss: 45.0646\n",
      "Epoch [114/300], Train loss: 2115.4819, Validation loss: 180.1297\n",
      "Epoch [115/300], Train loss: 158.0345, Validation loss: 35.9054\n",
      "Epoch [116/300], Train loss: 206.1843, Validation loss: 160.6616\n",
      "Epoch [117/300], Train loss: 205.7118, Validation loss: 51.9828\n",
      "Epoch [118/300], Train loss: 166.2318, Validation loss: 44.5172\n",
      "Epoch [119/300], Train loss: 234.4158, Validation loss: 80.2947\n",
      "Epoch [120/300], Train loss: 93939.5312, Validation loss: 119.5060\n",
      "Epoch [121/300], Train loss: 1680.8575, Validation loss: 3428.5154\n",
      "Epoch [122/300], Train loss: 1819.6711, Validation loss: 2115.7727\n",
      "Epoch [123/300], Train loss: 894.5561, Validation loss: 3193.4683\n",
      "Epoch [124/300], Train loss: 669.7632, Validation loss: 3054.6035\n",
      "Epoch [125/300], Train loss: 586.2789, Validation loss: 2546.4094\n",
      "Epoch [126/300], Train loss: 3107.7668, Validation loss: 1064.8904\n",
      "Epoch [127/300], Train loss: 227.9319, Validation loss: 523.2197\n",
      "Epoch [128/300], Train loss: 417322.1562, Validation loss: 1713.8134\n",
      "Epoch [129/300], Train loss: 733949.5000, Validation loss: 2150.9712\n",
      "Epoch [130/300], Train loss: 976.3942, Validation loss: 1972.8699\n",
      "Epoch [131/300], Train loss: 556.5030, Validation loss: 807.6913\n",
      "Epoch [132/300], Train loss: 362.3312, Validation loss: 346.4628\n",
      "Epoch [133/300], Train loss: 1755.3359, Validation loss: 665.2826\n",
      "Epoch [134/300], Train loss: 39.4075, Validation loss: 62.9401\n",
      "Epoch [135/300], Train loss: 620.2089, Validation loss: 91.7760\n",
      "Epoch [136/300], Train loss: 36.8691, Validation loss: 48.1296\n",
      "Epoch [137/300], Train loss: 97.6887, Validation loss: 57.7733\n",
      "Epoch [138/300], Train loss: 35.4567, Validation loss: 38.1307\n",
      "Epoch [139/300], Train loss: 174.8635, Validation loss: 31.9137\n",
      "Epoch [140/300], Train loss: 4340.8003, Validation loss: 49.1638\n",
      "Epoch [141/300], Train loss: 193609.2812, Validation loss: 41.7995\n",
      "Epoch [142/300], Train loss: 6580.0781, Validation loss: 286.5642\n",
      "Epoch [143/300], Train loss: 40.5379, Validation loss: 46.7905\n",
      "Epoch [144/300], Train loss: 81.0226, Validation loss: 301.1096\n",
      "Epoch [145/300], Train loss: 80939.3125, Validation loss: 408.0910\n",
      "Epoch [146/300], Train loss: 154667.6250, Validation loss: 16.1968\n",
      "Epoch [147/300], Train loss: 34.3200, Validation loss: 58.5568\n",
      "Epoch [148/300], Train loss: 1701.7323, Validation loss: 16.3479\n",
      "Epoch [149/300], Train loss: 313.8686, Validation loss: 366.3337\n",
      "Epoch [150/300], Train loss: 157.5846, Validation loss: 95.1069\n",
      "Epoch [151/300], Train loss: 48815.6953, Validation loss: 164.4736\n",
      "Epoch [152/300], Train loss: 8002.1348, Validation loss: 93.0747\n",
      "Epoch [153/300], Train loss: 15.2990, Validation loss: 15.0094\n",
      "Epoch [154/300], Train loss: 73.8447, Validation loss: 126.7255\n",
      "Epoch [155/300], Train loss: 135.4956, Validation loss: 16.1157\n",
      "Epoch [156/300], Train loss: 43.2598, Validation loss: 15.4268\n",
      "Epoch [157/300], Train loss: 27.0060, Validation loss: 22.1818\n",
      "Epoch [158/300], Train loss: 182.4900, Validation loss: 32.6469\n",
      "Epoch [159/300], Train loss: 246.5151, Validation loss: 17.7794\n",
      "Epoch [160/300], Train loss: 5159.6245, Validation loss: 25.0167\n",
      "Epoch [161/300], Train loss: 79.9380, Validation loss: 41.0296\n",
      "Epoch [162/300], Train loss: 135.3406, Validation loss: 17.7037\n",
      "Epoch [163/300], Train loss: 169.6601, Validation loss: 44.3776\n",
      "Epoch [164/300], Train loss: 52.4466, Validation loss: 26.5606\n",
      "Epoch [165/300], Train loss: 58.8408, Validation loss: 79.2701\n",
      "Epoch [166/300], Train loss: 57.9286, Validation loss: 26.1323\n",
      "Epoch [167/300], Train loss: 109.9638, Validation loss: 27.1860\n",
      "Epoch [168/300], Train loss: 33.1434, Validation loss: 90.1429\n",
      "Epoch [169/300], Train loss: 17.6069, Validation loss: 34.3988\n",
      "Epoch [170/300], Train loss: 105.8417, Validation loss: 26.7522\n",
      "Epoch [171/300], Train loss: 52.3088, Validation loss: 57.4236\n",
      "Epoch [172/300], Train loss: 73.7334, Validation loss: 21.6027\n",
      "Epoch [173/300], Train loss: 197.8963, Validation loss: 24.0274\n",
      "Epoch [174/300], Train loss: 390.4163, Validation loss: 38.6942\n",
      "Epoch [175/300], Train loss: 30.8705, Validation loss: 49.7456\n",
      "Epoch [176/300], Train loss: 216.8515, Validation loss: 33.5305\n",
      "Epoch [177/300], Train loss: 105.9185, Validation loss: 172.5038\n",
      "Epoch [178/300], Train loss: 70.5099, Validation loss: 18.3642\n",
      "Epoch [179/300], Train loss: 361.8554, Validation loss: 120.5828\n",
      "Epoch [180/300], Train loss: 144.4983, Validation loss: 22.5021\n",
      "Epoch [181/300], Train loss: 312.5397, Validation loss: 44.9455\n",
      "Epoch [182/300], Train loss: 84.9166, Validation loss: 24.1414\n",
      "Epoch [183/300], Train loss: 69.3064, Validation loss: 19.8166\n",
      "Epoch [184/300], Train loss: 35.0691, Validation loss: 15.1652\n",
      "Epoch [185/300], Train loss: 45.2683, Validation loss: 14.2213\n",
      "Epoch [186/300], Train loss: 56.6584, Validation loss: 11.3082\n",
      "Epoch [187/300], Train loss: 86.4155, Validation loss: 15.9283\n",
      "Epoch [188/300], Train loss: 482.8881, Validation loss: 13.8572\n",
      "Epoch [189/300], Train loss: 23.7853, Validation loss: 9.4728\n",
      "Epoch [190/300], Train loss: 88.9501, Validation loss: 20.7632\n",
      "Epoch [191/300], Train loss: 7.6125, Validation loss: 12.3007\n",
      "Epoch [192/300], Train loss: 44.9305, Validation loss: 13.9860\n",
      "Epoch [193/300], Train loss: 98.9930, Validation loss: 8.1584\n",
      "Epoch [194/300], Train loss: 10.7676, Validation loss: 13.3588\n",
      "Epoch [195/300], Train loss: 6.9616, Validation loss: 10.6457\n",
      "Epoch [196/300], Train loss: 58.6005, Validation loss: 20.9266\n",
      "Epoch [197/300], Train loss: 22.4147, Validation loss: 6.6980\n",
      "Epoch [198/300], Train loss: 47.0352, Validation loss: 6.0835\n",
      "Epoch [199/300], Train loss: 4.7513, Validation loss: 19.5826\n",
      "Epoch [200/300], Train loss: 11.2965, Validation loss: 12.8029\n",
      "Epoch [201/300], Train loss: 66.4677, Validation loss: 14.1446\n",
      "Epoch [202/300], Train loss: 56.9331, Validation loss: 124.6011\n",
      "Epoch [203/300], Train loss: 9.1237, Validation loss: 9.9239\n",
      "Epoch [204/300], Train loss: 97.0580, Validation loss: 108.0010\n",
      "Epoch [205/300], Train loss: 199.4542, Validation loss: 32.8769\n",
      "Epoch [206/300], Train loss: 29.5461, Validation loss: 12.8407\n",
      "Epoch [207/300], Train loss: 62.4866, Validation loss: 35.8382\n",
      "Epoch [208/300], Train loss: 7.6924, Validation loss: 32.8808\n",
      "Epoch [209/300], Train loss: 63.6880, Validation loss: 34.6235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [210/300], Train loss: 140.9042, Validation loss: 34.7817\n",
      "Epoch [211/300], Train loss: 168.9182, Validation loss: 16.7787\n",
      "Epoch [212/300], Train loss: 7.4158, Validation loss: 13.0677\n",
      "Epoch [213/300], Train loss: 69.1903, Validation loss: 7.9925\n",
      "Epoch [214/300], Train loss: 4.6753, Validation loss: 7.4068\n",
      "Epoch [215/300], Train loss: 244.2329, Validation loss: 5.8324\n",
      "Epoch [216/300], Train loss: 52.7212, Validation loss: 5.7102\n",
      "Epoch [217/300], Train loss: 48.3909, Validation loss: 4.7904\n",
      "Epoch [218/300], Train loss: 16.9241, Validation loss: 6.2846\n",
      "Epoch [219/300], Train loss: 12.3179, Validation loss: 9.8961\n",
      "Epoch [220/300], Train loss: 9.7051, Validation loss: 10.3496\n",
      "Epoch [221/300], Train loss: 17.0234, Validation loss: 10.7501\n",
      "Epoch [222/300], Train loss: 22.6309, Validation loss: 6.9399\n",
      "Epoch [223/300], Train loss: 4.9160, Validation loss: 5.9217\n",
      "Epoch [224/300], Train loss: 12.9708, Validation loss: 12.5576\n",
      "Epoch [225/300], Train loss: 55.4032, Validation loss: 13.2838\n",
      "Epoch [226/300], Train loss: 36.7118, Validation loss: 14.2123\n",
      "Epoch [227/300], Train loss: 32.8103, Validation loss: 24.6986\n",
      "Epoch [228/300], Train loss: 6067.7866, Validation loss: 17.6261\n",
      "Epoch [229/300], Train loss: 29.5303, Validation loss: 31.5048\n",
      "Epoch [230/300], Train loss: 24.1124, Validation loss: 15.3866\n",
      "Epoch [231/300], Train loss: 4.7631, Validation loss: 17.7789\n",
      "Epoch [232/300], Train loss: 47.3803, Validation loss: 19.3606\n",
      "Epoch [233/300], Train loss: 117.8817, Validation loss: 12.1207\n",
      "Epoch [234/300], Train loss: 49.7514, Validation loss: 22.5800\n",
      "Epoch [235/300], Train loss: 6241.8296, Validation loss: 12.2910\n",
      "Epoch [236/300], Train loss: 6774.9165, Validation loss: 20.3435\n",
      "Epoch [237/300], Train loss: 17.1417, Validation loss: 28.5119\n",
      "Epoch [238/300], Train loss: 36.0941, Validation loss: 134.6165\n",
      "Epoch [239/300], Train loss: 44.0507, Validation loss: 4.9679\n",
      "Epoch [240/300], Train loss: 42.9478, Validation loss: 53.6745\n",
      "Epoch [241/300], Train loss: 55.0962, Validation loss: 3.3328\n",
      "Epoch [242/300], Train loss: 63.5272, Validation loss: 77.2059\n",
      "Epoch [243/300], Train loss: 211.5207, Validation loss: 93.7940\n",
      "Epoch [244/300], Train loss: 46.1638, Validation loss: 30.7910\n",
      "Epoch [245/300], Train loss: 291.7819, Validation loss: 104.8444\n",
      "Epoch [246/300], Train loss: 56.9768, Validation loss: 12.0442\n",
      "Epoch [247/300], Train loss: 39.7431, Validation loss: 18.7354\n",
      "Epoch [248/300], Train loss: 94.9358, Validation loss: 7.7979\n",
      "Epoch [249/300], Train loss: 16.4672, Validation loss: 7.7630\n",
      "Epoch [250/300], Train loss: 117.4893, Validation loss: 22.9196\n",
      "Epoch [251/300], Train loss: 94.2794, Validation loss: 127.5437\n",
      "Epoch [252/300], Train loss: 11.0586, Validation loss: 2.2891\n",
      "Epoch [253/300], Train loss: 136.1958, Validation loss: 253.9007\n",
      "Epoch [254/300], Train loss: 21.5229, Validation loss: 6.5188\n",
      "Epoch [255/300], Train loss: 18.3657, Validation loss: 12.3547\n",
      "Epoch [256/300], Train loss: 41.1621, Validation loss: 11.4146\n",
      "Epoch [257/300], Train loss: 49.1660, Validation loss: 94.7418\n",
      "Epoch [258/300], Train loss: 69.2676, Validation loss: 48.6521\n",
      "Epoch [259/300], Train loss: 13745.9150, Validation loss: 128.5299\n",
      "Epoch [260/300], Train loss: 790.6115, Validation loss: 101.6086\n",
      "Epoch [261/300], Train loss: 213.4118, Validation loss: 399.6615\n",
      "Epoch [262/300], Train loss: 2311.7075, Validation loss: 1138.3461\n",
      "Epoch [263/300], Train loss: 16.9561, Validation loss: 80.4604\n",
      "Epoch [264/300], Train loss: 1315.2280, Validation loss: 1531.4407\n",
      "Epoch [265/300], Train loss: 142.0891, Validation loss: 278.1770\n",
      "Epoch [266/300], Train loss: 2069.6299, Validation loss: 1495.4109\n",
      "Epoch [267/300], Train loss: 80.3808, Validation loss: 302.7064\n",
      "Epoch [268/300], Train loss: 48.7666, Validation loss: 204.0901\n",
      "Epoch [269/300], Train loss: 47.5258, Validation loss: 143.7032\n",
      "Epoch [270/300], Train loss: 43.3817, Validation loss: 122.3586\n",
      "Epoch [271/300], Train loss: 277.1063, Validation loss: 95.4862\n",
      "Epoch [272/300], Train loss: 68.1630, Validation loss: 21.9159\n",
      "Epoch [273/300], Train loss: 23.2930, Validation loss: 111.9839\n",
      "Epoch [274/300], Train loss: 198.1768, Validation loss: 13.3955\n",
      "Epoch [275/300], Train loss: 52.1760, Validation loss: 12.4261\n",
      "Epoch [276/300], Train loss: 12899.0742, Validation loss: 35.2439\n",
      "Epoch [277/300], Train loss: 14259.5205, Validation loss: 281.9780\n",
      "Epoch [278/300], Train loss: 181.2950, Validation loss: 88.1968\n",
      "Epoch [279/300], Train loss: 1098.8108, Validation loss: 1347.4454\n",
      "Epoch [280/300], Train loss: 1046.6580, Validation loss: 884.3034\n",
      "Epoch [281/300], Train loss: 111.2417, Validation loss: 17.7631\n",
      "Epoch [282/300], Train loss: 37710.8008, Validation loss: 640.0828\n",
      "Epoch [283/300], Train loss: 183.9392, Validation loss: 79.9503\n",
      "Epoch [284/300], Train loss: 230.5715, Validation loss: 68.5205\n",
      "Epoch [285/300], Train loss: 520.5495, Validation loss: 293.1378\n",
      "Epoch [286/300], Train loss: 58701.7930, Validation loss: 550.6253\n",
      "Epoch [287/300], Train loss: 180081.0469, Validation loss: 518.8095\n",
      "Epoch [288/300], Train loss: 130.7978, Validation loss: 560.2098\n",
      "Epoch [289/300], Train loss: 5775.9600, Validation loss: 543.5542\n",
      "Epoch [290/300], Train loss: 212.5710, Validation loss: 2133.0208\n",
      "Epoch [291/300], Train loss: 809.3372, Validation loss: 1616.4231\n",
      "Epoch [292/300], Train loss: 320566.5000, Validation loss: 1016.5975\n",
      "Epoch [293/300], Train loss: 310.4399, Validation loss: 447.5327\n",
      "Epoch [294/300], Train loss: 89.2410, Validation loss: 160.6895\n",
      "Epoch [295/300], Train loss: 2221.2954, Validation loss: 682.3925\n",
      "Epoch [296/300], Train loss: 776.8759, Validation loss: 325.7865\n",
      "Epoch [297/300], Train loss: 1037.6272, Validation loss: 286.8100\n",
      "Epoch [298/300], Train loss: 45.8929, Validation loss: 42.5392\n",
      "Epoch [299/300], Train loss: 81.1517, Validation loss: 11.5925\n",
      "Epoch [300/300], Train loss: 61.5261, Validation loss: 16.6404\n",
      "lowest validation error achieved : 2.2891\n",
      "test error:  10.958985005704893\n",
      "\n",
      "\n",
      "CV-3:\n",
      "----------------\n",
      "Epoch [1/300], Train loss: 311.1206, Validation loss: 74977.6250\n",
      "Epoch [2/300], Train loss: 722.9414, Validation loss: 10813.5322\n",
      "Epoch [3/300], Train loss: 995.1985, Validation loss: 4925.6099\n",
      "Epoch [4/300], Train loss: 689103.6250, Validation loss: 376835.1250\n",
      "Epoch [5/300], Train loss: 232.0787, Validation loss: 6661.5845\n",
      "Epoch [6/300], Train loss: 5680.3213, Validation loss: 6390.1494\n",
      "Epoch [7/300], Train loss: 1432.0107, Validation loss: 5646.8208\n",
      "Epoch [8/300], Train loss: 86.0216, Validation loss: 5501.3726\n",
      "Epoch [9/300], Train loss: 143.9666, Validation loss: 4673.3042\n",
      "Epoch [10/300], Train loss: 42.1349, Validation loss: 4793.2393\n",
      "Epoch [11/300], Train loss: 424.2951, Validation loss: 7944.8379\n",
      "Epoch [12/300], Train loss: 7343.3789, Validation loss: 37125.8516\n",
      "Epoch [13/300], Train loss: 1330.0504, Validation loss: 24223.4961\n",
      "Epoch [14/300], Train loss: 3500.5425, Validation loss: 3893.0398\n",
      "Epoch [15/300], Train loss: 564.2296, Validation loss: 6223.5718\n",
      "Epoch [16/300], Train loss: 842.5762, Validation loss: 1797.0540\n",
      "Epoch [17/300], Train loss: 4352.7617, Validation loss: 2238.1196\n",
      "Epoch [18/300], Train loss: 12888.2861, Validation loss: 6967.1421\n",
      "Epoch [19/300], Train loss: 3164.7087, Validation loss: 4253.2695\n",
      "Epoch [20/300], Train loss: 6856.8110, Validation loss: 62299.9805\n",
      "Epoch [21/300], Train loss: 1030.1002, Validation loss: 8416.3555\n",
      "Epoch [22/300], Train loss: 2611.7197, Validation loss: 5050.0698\n",
      "Epoch [23/300], Train loss: 1620.5018, Validation loss: 5022.0415\n",
      "Epoch [24/300], Train loss: 259.4934, Validation loss: 6227.8657\n",
      "Epoch [25/300], Train loss: 1189.6184, Validation loss: 7634.9487\n",
      "Epoch [26/300], Train loss: 194.8186, Validation loss: 2639.5886\n",
      "Epoch [27/300], Train loss: 826.1962, Validation loss: 1537.1890\n",
      "Epoch [28/300], Train loss: 1077.0187, Validation loss: 5304.7393\n",
      "Epoch [29/300], Train loss: 2403.8442, Validation loss: 49365.8672\n",
      "Epoch [30/300], Train loss: 267382.5625, Validation loss: 34524.3086\n",
      "Epoch [31/300], Train loss: 11861.0176, Validation loss: 114478.5078\n",
      "Epoch [32/300], Train loss: 182.1717, Validation loss: 5075.5371\n",
      "Epoch [33/300], Train loss: 38926.9961, Validation loss: 5914.3613\n",
      "Epoch [34/300], Train loss: 188.9297, Validation loss: 6547.2505\n",
      "Epoch [35/300], Train loss: 3830.5823, Validation loss: 9593.0684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/300], Train loss: 338.5067, Validation loss: 10940.5576\n",
      "Epoch [37/300], Train loss: 125.8683, Validation loss: 14406.9697\n",
      "Epoch [38/300], Train loss: 68.4023, Validation loss: 10288.3223\n",
      "Epoch [39/300], Train loss: 172.2272, Validation loss: 1829.0637\n",
      "Epoch [40/300], Train loss: 885.8872, Validation loss: 3081.2239\n",
      "Epoch [41/300], Train loss: 3347.7300, Validation loss: 6874.7305\n",
      "Epoch [42/300], Train loss: 1024.1449, Validation loss: 5317.7769\n",
      "Epoch [43/300], Train loss: 1012.0021, Validation loss: 3614.3525\n",
      "Epoch [44/300], Train loss: 1210.3983, Validation loss: 2819.5557\n",
      "Epoch [45/300], Train loss: 12796.0684, Validation loss: 2204.7637\n",
      "Epoch [46/300], Train loss: 1015.3221, Validation loss: 2874.5295\n",
      "Epoch [47/300], Train loss: 3630.8032, Validation loss: 2661.8306\n",
      "Epoch [48/300], Train loss: 38.6195, Validation loss: 2935.1411\n",
      "Epoch [49/300], Train loss: 6395.6504, Validation loss: 2823.0994\n",
      "Epoch [50/300], Train loss: 5369.0898, Validation loss: 1530.3193\n",
      "Epoch [51/300], Train loss: 376.2914, Validation loss: 2125.8232\n",
      "Epoch [52/300], Train loss: 276.2956, Validation loss: 1876.1898\n",
      "Epoch [53/300], Train loss: 356.4618, Validation loss: 1589.0502\n",
      "Epoch [54/300], Train loss: 30.7666, Validation loss: 2299.2007\n",
      "Epoch [55/300], Train loss: 192.9329, Validation loss: 2272.2817\n",
      "Epoch [56/300], Train loss: 365.3988, Validation loss: 1298.8291\n",
      "Epoch [57/300], Train loss: 542.8632, Validation loss: 1653.5557\n",
      "Epoch [58/300], Train loss: 8094.3340, Validation loss: 2703.0249\n",
      "Epoch [59/300], Train loss: 21042.3945, Validation loss: 4892.7510\n",
      "Epoch [60/300], Train loss: 457.1350, Validation loss: 2119.3914\n",
      "Epoch [61/300], Train loss: 358.1222, Validation loss: 955.2780\n",
      "Epoch [62/300], Train loss: 1500.1146, Validation loss: 3961.2673\n",
      "Epoch [63/300], Train loss: 53.7114, Validation loss: 1441.3341\n",
      "Epoch [64/300], Train loss: 302.1387, Validation loss: 1372.3497\n",
      "Epoch [65/300], Train loss: 558.4382, Validation loss: 1586.6893\n",
      "Epoch [66/300], Train loss: 858.2460, Validation loss: 2825.7935\n",
      "Epoch [67/300], Train loss: 595.9648, Validation loss: 2720.7542\n",
      "Epoch [68/300], Train loss: 134.3955, Validation loss: 9603.4639\n",
      "Epoch [69/300], Train loss: 65.3509, Validation loss: 2517.3530\n",
      "Epoch [70/300], Train loss: 11950.3564, Validation loss: 14827.7285\n",
      "Epoch [71/300], Train loss: 532.2468, Validation loss: 1177.2314\n",
      "Epoch [72/300], Train loss: 570.3630, Validation loss: 4993.0488\n",
      "Epoch [73/300], Train loss: 69.2713, Validation loss: 1992.8787\n",
      "Epoch [74/300], Train loss: 4431.7271, Validation loss: 1847.3319\n",
      "Epoch [75/300], Train loss: 193.8148, Validation loss: 2350.1174\n",
      "Epoch [76/300], Train loss: 203.6590, Validation loss: 1620.2289\n",
      "Epoch [77/300], Train loss: 347.6772, Validation loss: 1520.5906\n",
      "Epoch [78/300], Train loss: 61.5155, Validation loss: 2806.8005\n",
      "Epoch [79/300], Train loss: 159.2363, Validation loss: 1157.5415\n",
      "Epoch [80/300], Train loss: 309.9466, Validation loss: 1454.9194\n",
      "Epoch [81/300], Train loss: 734.0300, Validation loss: 1622.8256\n",
      "Epoch [82/300], Train loss: 474.4027, Validation loss: 913.6460\n",
      "Epoch [83/300], Train loss: 2949.5566, Validation loss: 879.4470\n",
      "Epoch [84/300], Train loss: 109.2325, Validation loss: 1145.6714\n",
      "Epoch [85/300], Train loss: 23.0927, Validation loss: 828.6198\n",
      "Epoch [86/300], Train loss: 70.2266, Validation loss: 790.6933\n",
      "Epoch [87/300], Train loss: 59.8746, Validation loss: 920.3770\n",
      "Epoch [88/300], Train loss: 122.2721, Validation loss: 841.8880\n",
      "Epoch [89/300], Train loss: 99.5827, Validation loss: 764.0978\n",
      "Epoch [90/300], Train loss: 45.0257, Validation loss: 750.8163\n",
      "Epoch [91/300], Train loss: 758.1158, Validation loss: 750.4422\n",
      "Epoch [92/300], Train loss: 83.0647, Validation loss: 750.6818\n",
      "Epoch [93/300], Train loss: 1316.9933, Validation loss: 682.9090\n",
      "Epoch [94/300], Train loss: 47.9035, Validation loss: 655.1442\n",
      "Epoch [95/300], Train loss: 112.8064, Validation loss: 692.3477\n",
      "Epoch [96/300], Train loss: 234.7250, Validation loss: 677.0242\n",
      "Epoch [97/300], Train loss: 69.0117, Validation loss: 573.3674\n",
      "Epoch [98/300], Train loss: 42.3922, Validation loss: 690.0734\n",
      "Epoch [99/300], Train loss: 404.2986, Validation loss: 783.2226\n",
      "Epoch [100/300], Train loss: 23.6964, Validation loss: 467.5453\n",
      "Epoch [101/300], Train loss: 77.9107, Validation loss: 485.9026\n",
      "Epoch [102/300], Train loss: 234.8239, Validation loss: 1403.8660\n",
      "Epoch [103/300], Train loss: 228.2405, Validation loss: 665.9623\n",
      "Epoch [104/300], Train loss: 3054.1260, Validation loss: 598.3542\n",
      "Epoch [105/300], Train loss: 1306.1697, Validation loss: 733.3663\n",
      "Epoch [106/300], Train loss: 112.7020, Validation loss: 919.3928\n",
      "Epoch [107/300], Train loss: 53.5852, Validation loss: 682.2800\n",
      "Epoch [108/300], Train loss: 62.7300, Validation loss: 635.7123\n",
      "Epoch [109/300], Train loss: 29.5188, Validation loss: 875.2379\n",
      "Epoch [110/300], Train loss: 907.6131, Validation loss: 549.5574\n",
      "Epoch [111/300], Train loss: 22.6189, Validation loss: 653.2569\n",
      "Epoch [112/300], Train loss: 957.9055, Validation loss: 558.6995\n",
      "Epoch [113/300], Train loss: 240.7507, Validation loss: 833.5550\n",
      "Epoch [114/300], Train loss: 14.6310, Validation loss: 492.7644\n",
      "Epoch [115/300], Train loss: 145.7491, Validation loss: 386.3206\n",
      "Epoch [116/300], Train loss: 287.5515, Validation loss: 408.8712\n",
      "Epoch [117/300], Train loss: 109.8251, Validation loss: 766.5433\n",
      "Epoch [118/300], Train loss: 286.3142, Validation loss: 365.0047\n",
      "Epoch [119/300], Train loss: 144.3712, Validation loss: 355.2451\n",
      "Epoch [120/300], Train loss: 499.7293, Validation loss: 963.7791\n",
      "Epoch [121/300], Train loss: 244.8877, Validation loss: 330.0519\n",
      "Epoch [122/300], Train loss: 25.9908, Validation loss: 588.5657\n",
      "Epoch [123/300], Train loss: 1114.1449, Validation loss: 982.8969\n",
      "Epoch [124/300], Train loss: 146.1823, Validation loss: 330.2299\n",
      "Epoch [125/300], Train loss: 228.7096, Validation loss: 370.4472\n",
      "Epoch [126/300], Train loss: 5589.9810, Validation loss: 788.0513\n",
      "Epoch [127/300], Train loss: 432.0561, Validation loss: 536.3408\n",
      "Epoch [128/300], Train loss: 57.7513, Validation loss: 877.9444\n",
      "Epoch [129/300], Train loss: 84.6948, Validation loss: 531.9189\n",
      "Epoch [130/300], Train loss: 152.2968, Validation loss: 369.8849\n",
      "Epoch [131/300], Train loss: 127.5551, Validation loss: 1879.9988\n",
      "Epoch [132/300], Train loss: 22.3365, Validation loss: 2989.7588\n",
      "Epoch [133/300], Train loss: 48.3468, Validation loss: 754.4304\n",
      "Epoch [134/300], Train loss: 32.9311, Validation loss: 563.0566\n",
      "Epoch [135/300], Train loss: 92.7060, Validation loss: 13412.9424\n",
      "Epoch [136/300], Train loss: 2235.2671, Validation loss: 19933.0547\n",
      "Epoch [137/300], Train loss: 171.4272, Validation loss: 1442.5974\n",
      "Epoch [138/300], Train loss: 22.2175, Validation loss: 13340.2979\n",
      "Epoch [139/300], Train loss: 214869.9531, Validation loss: 8219.6689\n",
      "Epoch [140/300], Train loss: 2225.8264, Validation loss: 5974.4658\n",
      "Epoch [141/300], Train loss: 111.4277, Validation loss: 967.4605\n",
      "Epoch [142/300], Train loss: 460.1508, Validation loss: 2053.6089\n",
      "Epoch [143/300], Train loss: 350.7513, Validation loss: 1329.7856\n",
      "Epoch [144/300], Train loss: 237.4910, Validation loss: 1795.7184\n",
      "Epoch [145/300], Train loss: 48.5684, Validation loss: 1166.0189\n",
      "Epoch [146/300], Train loss: 26.2479, Validation loss: 1134.7406\n",
      "Epoch [147/300], Train loss: 22055.9082, Validation loss: 735.8599\n",
      "Epoch [148/300], Train loss: 40.0689, Validation loss: 1173.0349\n",
      "Epoch [149/300], Train loss: 5114.9844, Validation loss: 668.6080\n",
      "Epoch [150/300], Train loss: 26.0818, Validation loss: 869.2018\n",
      "Epoch [151/300], Train loss: 88.8591, Validation loss: 654.7163\n",
      "Epoch [152/300], Train loss: 9161.1641, Validation loss: 573.5960\n",
      "Epoch [153/300], Train loss: 89.3096, Validation loss: 1625.1366\n",
      "Epoch [154/300], Train loss: 164.6837, Validation loss: 546.1874\n",
      "Epoch [155/300], Train loss: 896.1146, Validation loss: 721.2323\n",
      "Epoch [156/300], Train loss: 966.1816, Validation loss: 806.0914\n",
      "Epoch [157/300], Train loss: 125.5207, Validation loss: 977.0715\n",
      "Epoch [158/300], Train loss: 119.3810, Validation loss: 555.7248\n",
      "Epoch [159/300], Train loss: 86.4818, Validation loss: 826.4669\n",
      "Epoch [160/300], Train loss: 195.2990, Validation loss: 657.5307\n",
      "Epoch [161/300], Train loss: 49.9708, Validation loss: 546.0486\n",
      "Epoch [162/300], Train loss: 85.1535, Validation loss: 503.1738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [163/300], Train loss: 74.9720, Validation loss: 595.7872\n",
      "Epoch [164/300], Train loss: 145.2076, Validation loss: 711.3098\n",
      "Epoch [165/300], Train loss: 284.3123, Validation loss: 693.2288\n",
      "Epoch [166/300], Train loss: 138.3975, Validation loss: 436.8673\n",
      "Epoch [167/300], Train loss: 292.9124, Validation loss: 606.6089\n",
      "Epoch [168/300], Train loss: 396.0034, Validation loss: 624.3257\n",
      "Epoch [169/300], Train loss: 22.5851, Validation loss: 574.1160\n",
      "Epoch [170/300], Train loss: 7658.6055, Validation loss: 664.2303\n",
      "Epoch [171/300], Train loss: 136.3677, Validation loss: 415.3444\n",
      "Epoch [172/300], Train loss: 80.4987, Validation loss: 437.4364\n",
      "Epoch [173/300], Train loss: 163.0217, Validation loss: 518.5482\n",
      "Epoch [174/300], Train loss: 1817.1843, Validation loss: 414.2203\n",
      "Epoch [175/300], Train loss: 312.7479, Validation loss: 461.3197\n",
      "Epoch [176/300], Train loss: 10.2742, Validation loss: 388.1077\n",
      "Epoch [177/300], Train loss: 50.3665, Validation loss: 420.9211\n",
      "Epoch [178/300], Train loss: 56.9413, Validation loss: 788.1681\n",
      "Epoch [179/300], Train loss: 143.0411, Validation loss: 364.0584\n",
      "Epoch [180/300], Train loss: 35.1274, Validation loss: 327.3492\n",
      "Epoch [181/300], Train loss: 34.4211, Validation loss: 859.7939\n",
      "Epoch [182/300], Train loss: 515.0052, Validation loss: 338.3253\n",
      "Epoch [183/300], Train loss: 119.4233, Validation loss: 459.4923\n",
      "Epoch [184/300], Train loss: 99.9282, Validation loss: 526.4108\n",
      "Epoch [185/300], Train loss: 72.4905, Validation loss: 437.7305\n",
      "Epoch [186/300], Train loss: 37.3082, Validation loss: 322.0642\n",
      "Epoch [187/300], Train loss: 218.6096, Validation loss: 620.0498\n",
      "Epoch [188/300], Train loss: 47.1841, Validation loss: 341.5456\n",
      "Epoch [189/300], Train loss: 9.2941, Validation loss: 353.7144\n",
      "Epoch [190/300], Train loss: 1153.0931, Validation loss: 538.0658\n",
      "Epoch [191/300], Train loss: 15.7157, Validation loss: 393.4848\n",
      "Epoch [192/300], Train loss: 78.2596, Validation loss: 348.6905\n",
      "Epoch [193/300], Train loss: 1306.4674, Validation loss: 354.5536\n",
      "Epoch [194/300], Train loss: 112.2822, Validation loss: 432.2195\n",
      "Epoch [195/300], Train loss: 9.5519, Validation loss: 335.2430\n",
      "Epoch [196/300], Train loss: 20.0477, Validation loss: 288.2943\n",
      "Epoch [197/300], Train loss: 38.9972, Validation loss: 337.4472\n",
      "Epoch [198/300], Train loss: 62.0046, Validation loss: 357.8242\n",
      "Epoch [199/300], Train loss: 73.8036, Validation loss: 411.5381\n",
      "Epoch [200/300], Train loss: 21.7852, Validation loss: 523.3180\n",
      "Epoch [201/300], Train loss: 101.8829, Validation loss: 269.8688\n",
      "Epoch [202/300], Train loss: 10.0119, Validation loss: 319.7178\n",
      "Epoch [203/300], Train loss: 9453.1992, Validation loss: 558.3224\n",
      "Epoch [204/300], Train loss: 47.1952, Validation loss: 319.2184\n",
      "Epoch [205/300], Train loss: 52.1993, Validation loss: 309.5569\n",
      "Epoch [206/300], Train loss: 225.9635, Validation loss: 509.4329\n",
      "Epoch [207/300], Train loss: 44.0298, Validation loss: 314.1244\n",
      "Epoch [208/300], Train loss: 76.5360, Validation loss: 339.1307\n",
      "Epoch [209/300], Train loss: 101.4552, Validation loss: 310.1987\n",
      "Epoch [210/300], Train loss: 110.9203, Validation loss: 360.1363\n",
      "Epoch [211/300], Train loss: 16.0721, Validation loss: 262.4816\n",
      "Epoch [212/300], Train loss: 18.3709, Validation loss: 258.7019\n",
      "Epoch [213/300], Train loss: 118.0357, Validation loss: 466.5495\n",
      "Epoch [214/300], Train loss: 15014.4863, Validation loss: 442.0514\n",
      "Epoch [215/300], Train loss: 16.4737, Validation loss: 305.9266\n",
      "Epoch [216/300], Train loss: 107.0221, Validation loss: 578.5046\n",
      "Epoch [217/300], Train loss: 20.7631, Validation loss: 332.9305\n",
      "Epoch [218/300], Train loss: 27.6611, Validation loss: 270.8249\n",
      "Epoch [219/300], Train loss: 45.8034, Validation loss: 301.9063\n",
      "Epoch [220/300], Train loss: 9.4186, Validation loss: 284.4718\n",
      "Epoch [221/300], Train loss: 32.4830, Validation loss: 379.4337\n",
      "Epoch [222/300], Train loss: 25.0082, Validation loss: 280.3632\n",
      "Epoch [223/300], Train loss: 57.4057, Validation loss: 227.6170\n",
      "Epoch [224/300], Train loss: 49.1411, Validation loss: 428.9666\n",
      "Epoch [225/300], Train loss: 24.9488, Validation loss: 197.4027\n",
      "Epoch [226/300], Train loss: 54.1269, Validation loss: 226.1100\n",
      "Epoch [227/300], Train loss: 45.3898, Validation loss: 275.3148\n",
      "Epoch [228/300], Train loss: 15.9885, Validation loss: 219.7049\n",
      "Epoch [229/300], Train loss: 39.4529, Validation loss: 403.2633\n",
      "Epoch [230/300], Train loss: 489.7015, Validation loss: 213.4771\n",
      "Epoch [231/300], Train loss: 25166.1660, Validation loss: 203.3772\n",
      "Epoch [232/300], Train loss: 54.7714, Validation loss: 413.3494\n",
      "Epoch [233/300], Train loss: 89.7385, Validation loss: 227.8421\n",
      "Epoch [234/300], Train loss: 19.1094, Validation loss: 221.6268\n",
      "Epoch [235/300], Train loss: 37.8875, Validation loss: 269.0101\n",
      "Epoch [236/300], Train loss: 29.6587, Validation loss: 232.5290\n",
      "Epoch [237/300], Train loss: 8.1532, Validation loss: 241.7279\n",
      "Epoch [238/300], Train loss: 50.0885, Validation loss: 241.7391\n",
      "Epoch [239/300], Train loss: 62.7992, Validation loss: 222.7412\n",
      "Epoch [240/300], Train loss: 9.1036, Validation loss: 247.4538\n",
      "Epoch [241/300], Train loss: 7.1238, Validation loss: 229.9902\n",
      "Epoch [242/300], Train loss: 21.3559, Validation loss: 220.7163\n",
      "Epoch [243/300], Train loss: 111.2395, Validation loss: 338.0619\n",
      "Epoch [244/300], Train loss: 267.2141, Validation loss: 220.1791\n",
      "Epoch [245/300], Train loss: 56867.2266, Validation loss: 380.3962\n",
      "Epoch [246/300], Train loss: 5.1210, Validation loss: 407.5484\n",
      "Epoch [247/300], Train loss: 144.4100, Validation loss: 410.5911\n",
      "Epoch [248/300], Train loss: 17.4259, Validation loss: 239.1640\n",
      "Epoch [249/300], Train loss: 158.4672, Validation loss: 755.6479\n",
      "Epoch [250/300], Train loss: 32822.8828, Validation loss: 607.4462\n",
      "Epoch [251/300], Train loss: 209.3143, Validation loss: 514.8358\n",
      "Epoch [252/300], Train loss: 112.6576, Validation loss: 211.1367\n",
      "Epoch [253/300], Train loss: 44592.5078, Validation loss: 771.5079\n",
      "Epoch [254/300], Train loss: 324.6939, Validation loss: 1637.9425\n",
      "Epoch [255/300], Train loss: 186.0659, Validation loss: 2243.0225\n",
      "Epoch [256/300], Train loss: 46.1051, Validation loss: 298.5391\n",
      "Epoch [257/300], Train loss: 4015.9609, Validation loss: 1170.0767\n",
      "Epoch [258/300], Train loss: 111304.2891, Validation loss: 902.1923\n",
      "Epoch [259/300], Train loss: 2848.6106, Validation loss: 1529.4529\n",
      "Epoch [260/300], Train loss: 48.6120, Validation loss: 246.3376\n",
      "Epoch [261/300], Train loss: 40.7211, Validation loss: 354.1402\n",
      "Epoch [262/300], Train loss: 13.0349, Validation loss: 417.1582\n",
      "Epoch [263/300], Train loss: 26.7007, Validation loss: 236.3684\n",
      "Epoch [264/300], Train loss: 53.4704, Validation loss: 1343.8204\n",
      "Epoch [265/300], Train loss: 14.0919, Validation loss: 2012.4630\n",
      "Epoch [266/300], Train loss: 67.8258, Validation loss: 4061.9954\n",
      "Epoch [267/300], Train loss: 11.2620, Validation loss: 5300.3403\n",
      "Epoch [268/300], Train loss: 47.6802, Validation loss: 4015.7063\n",
      "Epoch [269/300], Train loss: 85.5595, Validation loss: 1312.3982\n",
      "Epoch [270/300], Train loss: 143.6660, Validation loss: 1017.2725\n",
      "Epoch [271/300], Train loss: 61.5000, Validation loss: 3739.0813\n",
      "Epoch [272/300], Train loss: 18.9502, Validation loss: 27981.6504\n",
      "Epoch [273/300], Train loss: 869890.1250, Validation loss: 70698.6250\n",
      "Epoch [274/300], Train loss: 2839.9434, Validation loss: 15314.7490\n",
      "Epoch [275/300], Train loss: 1945.2063, Validation loss: 13461.9912\n",
      "Epoch [276/300], Train loss: 894.8929, Validation loss: 8895.1787\n",
      "Epoch [277/300], Train loss: 846.5218, Validation loss: 10260.6846\n",
      "Epoch [278/300], Train loss: 2660.3567, Validation loss: 8726.6709\n",
      "Epoch [279/300], Train loss: 1279.0405, Validation loss: 11614.9775\n",
      "Epoch [280/300], Train loss: 4476.8101, Validation loss: 17718.6914\n",
      "Epoch [281/300], Train loss: 3549.5361, Validation loss: 11696.9707\n",
      "Epoch [282/300], Train loss: 18450.0098, Validation loss: 16562.3867\n",
      "Epoch [283/300], Train loss: 198.1732, Validation loss: 22166.9609\n",
      "Epoch [284/300], Train loss: 61.0458, Validation loss: 2119.0698\n",
      "Epoch [285/300], Train loss: 245.0115, Validation loss: 1566.8560\n",
      "Epoch [286/300], Train loss: 8891.8154, Validation loss: 8731.0186\n",
      "Epoch [287/300], Train loss: 87.5654, Validation loss: 1937.7455\n",
      "Epoch [288/300], Train loss: 2681.7480, Validation loss: 2177.0562\n",
      "Epoch [289/300], Train loss: 1752.5142, Validation loss: 2853.4700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [290/300], Train loss: 87.4022, Validation loss: 3601.7212\n",
      "Epoch [291/300], Train loss: 1311.7592, Validation loss: 3751.1069\n",
      "Epoch [292/300], Train loss: 173.5644, Validation loss: 9315.7979\n",
      "Epoch [293/300], Train loss: 390.3502, Validation loss: 12847.2041\n",
      "Epoch [294/300], Train loss: 394.7887, Validation loss: 13538.6055\n",
      "Epoch [295/300], Train loss: 3369.5859, Validation loss: 6101.4111\n",
      "Epoch [296/300], Train loss: 288.0942, Validation loss: 1687.7939\n",
      "Epoch [297/300], Train loss: 561.7949, Validation loss: 2935.0803\n",
      "Epoch [298/300], Train loss: 794.8951, Validation loss: 1370.9696\n",
      "Epoch [299/300], Train loss: 1582.4408, Validation loss: 4757.5024\n",
      "Epoch [300/300], Train loss: 118.4967, Validation loss: 937.6154\n",
      "lowest validation error achieved : 197.4027\n",
      "test error:  202.6339721351057\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "losses_train = []\n",
    "losses_val = []\n",
    "epochs = np.arange(1, num_epochs + 1)\n",
    "final_output_test = torch.Tensor(55, 1)\n",
    "cv_error = []\n",
    "for r in range(3):\n",
    "    irnet = IRNet().to(device)\n",
    "    optimizer = torch.optim.Adam(irnet.parameters(), lr=learning_rate)\n",
    "    \n",
    "    minLoss = 1000000000000.0\n",
    "    \n",
    "    train, test = train_test_split(df, test_size=0.25, random_state=r*2, shuffle=True)\n",
    "    test, val = train_test_split(test, test_size=0.4, random_state=r*2, shuffle=True)\n",
    "    \n",
    "    y_train = train['y-exp'].values\n",
    "    excluded_columns = [\"Name\"]\n",
    "    X_train = train.drop(excluded_columns, axis=1)\n",
    "\n",
    "    y_test = test['y-exp'].values\n",
    "    excluded_columns = [\"Name\"]\n",
    "    X_test = test.drop(excluded_columns, axis=1)\n",
    "\n",
    "    y_val = test['y-exp'].values\n",
    "    excluded_columns = [\"Name\"]\n",
    "    X_val = test.drop(excluded_columns, axis=1)\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    \n",
    "    y_train = torch.from_numpy(y_train)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "    y_val = torch.from_numpy(y_val)\n",
    "    #y_train = y_train.view(y_train.shape[0], 1)\n",
    "    y_test = y_test.view(y_test.shape[0], 1)\n",
    "    y_val = y_val.view(y_val.shape[0], 1)\n",
    "    \n",
    "    X_train = torch.from_numpy(X_train)\n",
    "    X_test = torch.from_numpy(X_test)\n",
    "    X_val = torch.from_numpy(X_val)\n",
    "    \n",
    "    train_dataset = Data.TensorDataset(X_train, y_train)\n",
    "    train_loader = Data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
    "    \n",
    "    #X_train = Variable(X_train).to(device)\n",
    "    X_test = Variable(X_test).to(device)\n",
    "    X_val = Variable(X_val).to(device)\n",
    "    #y_train = Variable(y_train).to(device)\n",
    "    y_test = Variable(y_test).to(device)\n",
    "    y_val = Variable(y_val).to(device)\n",
    "    \n",
    "    print(f'CV-{r + 1}:\\n----------------')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (batch_X, batch_y) in enumerate(train_loader):\n",
    "            #print(batch_X, batch_y)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            Xtor = Variable(batch_X).to(device)\n",
    "            ytor = Variable(batch_y.view(batch_y.shape[0], 1)).to(device)\n",
    "            #print(Xtor.shape, ytor.shape)\n",
    "\n",
    "            outputs = irnet(Xtor.float())\n",
    "            val_outputs = irnet(X_val.float())\n",
    "\n",
    "            loss = criterion(outputs, ytor.float())\n",
    "            val_loss = criterion(val_outputs, y_val.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('Epoch [%d/%d], Train loss: %.4f, Validation loss: %.4f'\n",
    "             %(epoch+1, num_epochs, loss.data, val_loss.data))\n",
    "        \n",
    "        if r == 0:\n",
    "            losses_train.append(loss)\n",
    "            losses_val.append(val_loss)\n",
    "\n",
    "        if val_loss.data < minLoss:\n",
    "            minLoss = val_loss.data\n",
    "            #print('%.4f'%(minLoss))\n",
    "            final_output_test = irnet(X_test.float())\n",
    "\n",
    "    print('lowest validation error achieved : %.4f'%(minLoss))\n",
    "    test_mse = mean_squared_error(y_test.cpu().detach().numpy(), final_output_test.cpu().detach().numpy())\n",
    "    cv_error.append(test_mse)\n",
    "    print('test error: ', test_mse)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV error :  97.62043552282408\n"
     ]
    }
   ],
   "source": [
    "print(\"CV error : \", np.mean(cv_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(len(losses_train)):\n",
    "    losses_train[i] = losses_train[i].cpu().detach().numpy()\n",
    "    losses_val[i] = losses_val[i].cpu().detach().numpy()\n",
    "#     losses_train[i] = losses_train[i]\n",
    "#     losses_val[i] = losses_val[i]\n",
    "plt.plot(epochs, losses_train)\n",
    "plt.plot(epochs, losses_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Results on test set after %d iterations:\\n------------------------------------------'%(num_epochs))\n",
    "# #print('Mean Absolute Error:', criterion(final_output_test, y_test.float()).cpu().detach().numpy())\n",
    "# #print('r2 score: ', r2_score(y_test.cpu().detach().numpy(), final_output_test.cpu().detach().numpy()))\n",
    "# print('Mean Squared Error: ', mean_squared_error(y_test.cpu().detach().numpy(), final_output_test.cpu().detach().numpy()))\n",
    "# #print('Root mean squared error: ', np.sqrt(mean_squared_error(y_test.cpu().detach().numpy(), final_output_test.cpu().detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "sns.set_theme(color_codes=True)\n",
    "ax = sns.regplot(x=y_test.cpu().detach().numpy(), y=final_output_test.cpu().detach().numpy(), line_kws={\"color\": \"black\"})\n",
    "plt.xlabel('targets')\n",
    "plt.ylabel('predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses_train = []\n",
    "# losses_val = []\n",
    "# epochs = []\n",
    "# minLoss = 1000000000000.0\n",
    "# final_output_test = torch.Tensor(X_test.shape[0], 1)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     for i, (batch_X, batch_y) in enumerate(train_loader):\n",
    "#         #print(batch_X, batch_y)\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         Xtor = Variable(batch_X).to(device)\n",
    "#         ytor = Variable(batch_y.view(batch_y.shape[0], 1)).to(device)\n",
    "#         #print(Xtor.shape, ytor.shape)\n",
    "        \n",
    "#         outputs = irnet(Xtor.float())\n",
    "#         val_outputs = irnet(X_val.float())\n",
    "        \n",
    "#         loss = criterion(outputs, ytor.float())\n",
    "#         val_loss = criterion(val_outputs, y_val.float())\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print('Epoch [%d/%d], Train loss: %.4f, Validation loss: %.4f'\n",
    "#          %(epoch+1, num_epochs, loss.data, val_loss.data))\n",
    "#     epochs.append(epoch)\n",
    "#     losses_train.append(loss)\n",
    "#     losses_val.append(val_loss)\n",
    "    \n",
    "#     if val_loss.data < minLoss:\n",
    "#         minLoss = val_loss.data\n",
    "#         #print('%.4f'%(minLoss))\n",
    "#         final_output_test = irnet(X_test.float())\n",
    "\n",
    "# print('lowest validation error achieved : %.4f'%(minLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
