{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import torch\n",
    "#print(torch.__version__)\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch.nn.init\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>V</th>\n",
       "      <th>M</th>\n",
       "      <th>n</th>\n",
       "      <th>np</th>\n",
       "      <th>B</th>\n",
       "      <th>G</th>\n",
       "      <th>E</th>\n",
       "      <th>v</th>\n",
       "      <th>H</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 43</th>\n",
       "      <th>Unnamed: 44</th>\n",
       "      <th>Unnamed: 45</th>\n",
       "      <th>Unnamed: 46</th>\n",
       "      <th>Unnamed: 47</th>\n",
       "      <th>Unnamed: 48</th>\n",
       "      <th>Unnamed: 49</th>\n",
       "      <th>Unnamed: 50</th>\n",
       "      <th>Unnamed: 51</th>\n",
       "      <th>Unnamed: 52</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66_C</td>\n",
       "      <td>11.41</td>\n",
       "      <td>24.022</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>431.7450</td>\n",
       "      <td>518.2490</td>\n",
       "      <td>1110.438653</td>\n",
       "      <td>0.071337</td>\n",
       "      <td>95.902307</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20012_InSb</td>\n",
       "      <td>73.73</td>\n",
       "      <td>236.570</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>36.8242</td>\n",
       "      <td>14.5179</td>\n",
       "      <td>38.494849</td>\n",
       "      <td>0.325772</td>\n",
       "      <td>3.219550</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1190_ZnSe</td>\n",
       "      <td>47.34</td>\n",
       "      <td>144.350</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>56.3189</td>\n",
       "      <td>29.0179</td>\n",
       "      <td>74.293904</td>\n",
       "      <td>0.280139</td>\n",
       "      <td>6.603032</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>682_NaF</td>\n",
       "      <td>24.74</td>\n",
       "      <td>41.988</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>52.4118</td>\n",
       "      <td>36.3275</td>\n",
       "      <td>88.528881</td>\n",
       "      <td>0.218483</td>\n",
       "      <td>10.653966</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16220_Si</td>\n",
       "      <td>40.97</td>\n",
       "      <td>56.170</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>90.0009</td>\n",
       "      <td>63.4097</td>\n",
       "      <td>154.050571</td>\n",
       "      <td>0.214724</td>\n",
       "      <td>15.043345</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Name      V        M  n  np         B         G            E  \\\n",
       "0        66_C  11.41   24.022  2   2  431.7450  518.2490  1110.438653   \n",
       "1  20012_InSb  73.73  236.570  2   2   36.8242   14.5179    38.494849   \n",
       "2   1190_ZnSe  47.34  144.350  2   2   56.3189   29.0179    74.293904   \n",
       "3     682_NaF  24.74   41.988  2   2   52.4118   36.3275    88.528881   \n",
       "4    16220_Si  40.97   56.170  2   2   90.0009   63.4097   154.050571   \n",
       "\n",
       "          v          H  ...  Unnamed: 43  Unnamed: 44  Unnamed: 45  \\\n",
       "0  0.071337  95.902307  ...          NaN          NaN          NaN   \n",
       "1  0.325772   3.219550  ...          NaN          NaN          NaN   \n",
       "2  0.280139   6.603032  ...          NaN          NaN          NaN   \n",
       "3  0.218483  10.653966  ...          NaN          NaN          NaN   \n",
       "4  0.214724  15.043345  ...          NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 46  Unnamed: 47  Unnamed: 48  Unnamed: 49  Unnamed: 50  \\\n",
       "0          NaN          NaN          NaN          NaN          NaN   \n",
       "1          NaN          NaN          NaN          NaN          NaN   \n",
       "2          NaN          NaN          NaN          NaN          NaN   \n",
       "3          NaN          NaN          NaN          NaN          NaN   \n",
       "4          NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 51  Unnamed: 52  \n",
       "0          NaN          NaN  \n",
       "1          NaN          NaN  \n",
       "2          NaN          NaN  \n",
       "3          NaN          NaN  \n",
       "4          NaN          NaN  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('thermal-dataset.xlsx', sheet_name ='dataset')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Name', 'V', 'M', 'n', 'np', 'B', 'G', 'E', 'v', 'H', 'B'', 'G'', 'ρ',\n",
       "       'vL', 'vS', 'va', 'Θe', 'γel', 'γes', 'γe', 'A', 'y-exp', 'y-theory',\n",
       "       'Formula', 'space group', 'k_BTE_DFT', 'k_BTE_DFT（1000 K）',\n",
       "       'k_AFLOW_AAPL', 'k_AFLOW_AGL', 'k_AFLOW_AGL (Poisson ratio σ=0.25)',\n",
       "       'k_Mingo', 'k_EXP', 'k_EXP（1000 K）', 'Unnamed: 33', 'Unnamed: 34',\n",
       "       'Unnamed: 35', 'Unnamed: 36', 'Unnamed: 37', 'Unnamed: 38',\n",
       "       'Unnamed: 39', 'Unnamed: 40', 'Unnamed: 41', 'Unnamed: 42',\n",
       "       'Unnamed: 43', 'Unnamed: 44', 'Unnamed: 45', 'Unnamed: 46',\n",
       "       'Unnamed: 47', 'Unnamed: 48', 'Unnamed: 49', 'Unnamed: 50',\n",
       "       'Unnamed: 51', 'Unnamed: 52'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Name', 'V', 'M', 'n', 'np', 'B', 'G', 'E', 'v', 'H', 'B'', 'G'', 'ρ',\n",
       "       'vL', 'vS', 'va', 'Θe', 'γel', 'γes', 'γe', 'A', 'y-exp', 'y-theory'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[:, :'y-theory']\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V</th>\n",
       "      <th>M</th>\n",
       "      <th>n</th>\n",
       "      <th>np</th>\n",
       "      <th>B</th>\n",
       "      <th>G</th>\n",
       "      <th>E</th>\n",
       "      <th>v</th>\n",
       "      <th>H</th>\n",
       "      <th>B'</th>\n",
       "      <th>...</th>\n",
       "      <th>vL</th>\n",
       "      <th>vS</th>\n",
       "      <th>va</th>\n",
       "      <th>Θe</th>\n",
       "      <th>γel</th>\n",
       "      <th>γes</th>\n",
       "      <th>γe</th>\n",
       "      <th>A</th>\n",
       "      <th>y-exp</th>\n",
       "      <th>y-theory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>3.700000e+02</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>145.503405</td>\n",
       "      <td>576.941738</td>\n",
       "      <td>6.810811</td>\n",
       "      <td>3.789189</td>\n",
       "      <td>111.257870</td>\n",
       "      <td>59.678087</td>\n",
       "      <td>147.105572</td>\n",
       "      <td>0.296581</td>\n",
       "      <td>10.521358</td>\n",
       "      <td>-6.982807</td>\n",
       "      <td>...</td>\n",
       "      <td>5.385817</td>\n",
       "      <td>2.928926</td>\n",
       "      <td>3.256445</td>\n",
       "      <td>263.253037</td>\n",
       "      <td>1.594866</td>\n",
       "      <td>1.119608</td>\n",
       "      <td>1.995672</td>\n",
       "      <td>9.121067e-05</td>\n",
       "      <td>44.079546</td>\n",
       "      <td>24.255426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>132.334810</td>\n",
       "      <td>625.505406</td>\n",
       "      <td>4.818313</td>\n",
       "      <td>3.150066</td>\n",
       "      <td>82.433063</td>\n",
       "      <td>77.165080</td>\n",
       "      <td>170.936177</td>\n",
       "      <td>0.083493</td>\n",
       "      <td>14.299973</td>\n",
       "      <td>11.632497</td>\n",
       "      <td>...</td>\n",
       "      <td>2.784979</td>\n",
       "      <td>1.973751</td>\n",
       "      <td>2.152686</td>\n",
       "      <td>225.042124</td>\n",
       "      <td>2.868737</td>\n",
       "      <td>4.816583</td>\n",
       "      <td>3.987999</td>\n",
       "      <td>9.802296e-04</td>\n",
       "      <td>277.377084</td>\n",
       "      <td>49.038768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>11.410000</td>\n",
       "      <td>8.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.904080</td>\n",
       "      <td>0.406580</td>\n",
       "      <td>1.215183</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.006197</td>\n",
       "      <td>-104.589000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.559351</td>\n",
       "      <td>0.331246</td>\n",
       "      <td>0.379108</td>\n",
       "      <td>25.916963</td>\n",
       "      <td>-7.029521</td>\n",
       "      <td>-65.475244</td>\n",
       "      <td>0.414990</td>\n",
       "      <td>1.459528e-07</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>0.026157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>53.210000</td>\n",
       "      <td>157.831500</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>47.720850</td>\n",
       "      <td>19.017575</td>\n",
       "      <td>49.639698</td>\n",
       "      <td>0.250683</td>\n",
       "      <td>3.580389</td>\n",
       "      <td>-8.717083</td>\n",
       "      <td>...</td>\n",
       "      <td>3.836785</td>\n",
       "      <td>1.889592</td>\n",
       "      <td>2.115443</td>\n",
       "      <td>144.201287</td>\n",
       "      <td>1.107011</td>\n",
       "      <td>0.624775</td>\n",
       "      <td>0.921418</td>\n",
       "      <td>9.899596e-07</td>\n",
       "      <td>3.887500</td>\n",
       "      <td>4.432038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>76.590000</td>\n",
       "      <td>287.557000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>92.003700</td>\n",
       "      <td>44.813700</td>\n",
       "      <td>114.970283</td>\n",
       "      <td>0.303591</td>\n",
       "      <td>7.326539</td>\n",
       "      <td>-2.906780</td>\n",
       "      <td>...</td>\n",
       "      <td>4.836986</td>\n",
       "      <td>2.496888</td>\n",
       "      <td>2.782292</td>\n",
       "      <td>210.531759</td>\n",
       "      <td>1.379069</td>\n",
       "      <td>1.027563</td>\n",
       "      <td>1.199428</td>\n",
       "      <td>1.864054e-06</td>\n",
       "      <td>11.075000</td>\n",
       "      <td>12.781238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>219.020000</td>\n",
       "      <td>907.187000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>160.493000</td>\n",
       "      <td>67.233975</td>\n",
       "      <td>175.339469</td>\n",
       "      <td>0.344524</td>\n",
       "      <td>11.691259</td>\n",
       "      <td>-1.296300</td>\n",
       "      <td>...</td>\n",
       "      <td>5.826099</td>\n",
       "      <td>3.203630</td>\n",
       "      <td>3.595112</td>\n",
       "      <td>281.975073</td>\n",
       "      <td>1.606329</td>\n",
       "      <td>1.373321</td>\n",
       "      <td>1.597086</td>\n",
       "      <td>3.706014e-06</td>\n",
       "      <td>23.082500</td>\n",
       "      <td>22.402090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>995.000000</td>\n",
       "      <td>5168.290000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>434.165000</td>\n",
       "      <td>523.852000</td>\n",
       "      <td>1120.785827</td>\n",
       "      <td>0.494396</td>\n",
       "      <td>106.084131</td>\n",
       "      <td>29.742400</td>\n",
       "      <td>...</td>\n",
       "      <td>18.019141</td>\n",
       "      <td>12.254442</td>\n",
       "      <td>13.361204</td>\n",
       "      <td>1755.800206</td>\n",
       "      <td>51.169090</td>\n",
       "      <td>47.244050</td>\n",
       "      <td>53.566903</td>\n",
       "      <td>1.670399e-02</td>\n",
       "      <td>5200.000000</td>\n",
       "      <td>547.103092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                V            M           n          np           B  \\\n",
       "count  370.000000   370.000000  370.000000  370.000000  370.000000   \n",
       "mean   145.503405   576.941738    6.810811    3.789189  111.257870   \n",
       "std    132.334810   625.505406    4.818313    3.150066   82.433063   \n",
       "min     11.410000     8.010000    2.000000    2.000000    7.904080   \n",
       "25%     53.210000   157.831500    3.000000    3.000000   47.720850   \n",
       "50%     76.590000   287.557000    5.000000    3.000000   92.003700   \n",
       "75%    219.020000   907.187000   12.000000    3.000000  160.493000   \n",
       "max    995.000000  5168.290000   29.000000   29.000000  434.165000   \n",
       "\n",
       "                G            E           v           H          B'  ...  \\\n",
       "count  370.000000   370.000000  370.000000  370.000000  370.000000  ...   \n",
       "mean    59.678087   147.105572    0.296581   10.521358   -6.982807  ...   \n",
       "std     77.165080   170.936177    0.083493   14.299973   11.632497  ...   \n",
       "min      0.406580     1.215183    0.000656    0.006197 -104.589000  ...   \n",
       "25%     19.017575    49.639698    0.250683    3.580389   -8.717083  ...   \n",
       "50%     44.813700   114.970283    0.303591    7.326539   -2.906780  ...   \n",
       "75%     67.233975   175.339469    0.344524   11.691259   -1.296300  ...   \n",
       "max    523.852000  1120.785827    0.494396  106.084131   29.742400  ...   \n",
       "\n",
       "               vL          vS          va           Θe         γel  \\\n",
       "count  370.000000  370.000000  370.000000   370.000000  370.000000   \n",
       "mean     5.385817    2.928926    3.256445   263.253037    1.594866   \n",
       "std      2.784979    1.973751    2.152686   225.042124    2.868737   \n",
       "min      1.559351    0.331246    0.379108    25.916963   -7.029521   \n",
       "25%      3.836785    1.889592    2.115443   144.201287    1.107011   \n",
       "50%      4.836986    2.496888    2.782292   210.531759    1.379069   \n",
       "75%      5.826099    3.203630    3.595112   281.975073    1.606329   \n",
       "max     18.019141   12.254442   13.361204  1755.800206   51.169090   \n",
       "\n",
       "              γes          γe             A        y-exp    y-theory  \n",
       "count  370.000000  370.000000  3.700000e+02   370.000000  370.000000  \n",
       "mean     1.119608    1.995672  9.121067e-05    44.079546   24.255426  \n",
       "std      4.816583    3.987999  9.802296e-04   277.377084   49.038768  \n",
       "min    -65.475244    0.414990  1.459528e-07     0.086000    0.026157  \n",
       "25%      0.624775    0.921418  9.899596e-07     3.887500    4.432038  \n",
       "50%      1.027563    1.199428  1.864054e-06    11.075000   12.781238  \n",
       "75%      1.373321    1.597086  3.706014e-06    23.082500   22.402090  \n",
       "max     47.244050   53.566903  1.670399e-02  5200.000000  547.103092  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(370, 23)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name        0\n",
       "V           0\n",
       "M           0\n",
       "n           0\n",
       "np          0\n",
       "B           0\n",
       "G           0\n",
       "E           0\n",
       "v           0\n",
       "H           0\n",
       "B'          0\n",
       "G'          0\n",
       "ρ           0\n",
       "vL          0\n",
       "vS          0\n",
       "va          0\n",
       "Θe          0\n",
       "γel         0\n",
       "γes         0\n",
       "γe          0\n",
       "A           0\n",
       "y-exp       0\n",
       "y-theory    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(len(df.n.unique()))\n",
    "print(len(df.np.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omee/anaconda3/lib/python3.8/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n",
      "/home/omee/anaconda3/lib/python3.8/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "#Replacing \"n\"\n",
    "X = df.loc[:, ['n']]\n",
    "y = df['y-exp']\n",
    "OHEncoder = ce.OneHotEncoder(cols = ['n'])\n",
    "newCols = OHEncoder.fit_transform(X, y)\n",
    "df = df.drop(['n'], axis = 1)\n",
    "df = df.join(newCols)\n",
    "\n",
    "#Replacing \"np\"\n",
    "X = df.loc[:, ['np']]\n",
    "y = df['y-exp']\n",
    "OHEncoder = ce.OneHotEncoder(cols = ['np'])\n",
    "newCols = OHEncoder.fit_transform(X, y)\n",
    "df = df.drop(['np'], axis = 1)\n",
    "df = df.join(newCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['Name', 'V', 'M', 'B', 'G', 'E', 'v', 'H', 'B'', 'G'', 'ρ', 'vL', 'vS',\n",
       "        'va', 'Θe', 'γel', 'γes', 'γe', 'A', 'y-exp', 'y-theory', 'n_1', 'n_2',\n",
       "        'n_3', 'n_4', 'n_5', 'n_6', 'n_7', 'n_8', 'n_9', 'n_10', 'n_11', 'n_12',\n",
       "        'n_13', 'n_14', 'n_15', 'np_1', 'np_2', 'np_3', 'np_4', 'np_5', 'np_6',\n",
       "        'np_7', 'np_8', 'np_9', 'np_10', 'np_11', 'np_12', 'np_13', 'np_14',\n",
       "        'np_15'],\n",
       "       dtype='object'),\n",
       " (51,))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns, df.columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.47103092e+02, 4.34409393e+00, 8.83552653e+00, 1.76556363e+01,\n",
       "       4.08157516e+01, 3.04336639e+01, 1.16520522e+01, 4.06207592e+01,\n",
       "       6.14393581e+01, 1.46431122e+02, 1.30123539e+02, 4.01184072e+01,\n",
       "       3.43295845e+02, 2.66374859e+01, 2.05159539e+01, 2.65594241e+02,\n",
       "       2.22351199e+02, 2.31233787e+02, 1.92571258e+01, 1.79763713e+02,\n",
       "       1.67790030e+02, 1.78481073e+02, 1.40004820e+02, 2.24286312e+01,\n",
       "       1.71740875e+02, 1.64527923e+02, 1.51845435e+02, 1.67923922e+02,\n",
       "       2.37071428e+01, 1.66205902e+02, 2.03897001e+02, 1.92243680e+02,\n",
       "       2.49756459e+01, 1.72200844e+01, 1.87350480e+01, 1.16504218e+01,\n",
       "       2.54374342e+01, 2.02139390e+01, 2.24774423e+01, 1.20080075e+02,\n",
       "       2.47966820e+01, 8.79382272e+01, 3.49949208e+01, 1.09468699e+01,\n",
       "       1.25784369e-01, 2.20510643e+01, 3.12421349e+00, 1.97465160e+01,\n",
       "       1.90527938e+01, 1.91718068e+00, 2.39450091e+01, 9.73866313e+00,\n",
       "       3.03845914e+01, 3.20414393e+01, 3.89541097e+01, 8.96149223e+00,\n",
       "       2.85669570e+01, 6.65112711e+00, 3.40508285e+01, 3.05842531e+01,\n",
       "       3.39689710e+01, 2.05507017e-01, 2.23224679e+01, 2.45587087e+01,\n",
       "       8.29762935e+00, 7.36260300e+00, 2.80103698e+01, 3.16154980e+01,\n",
       "       2.15320886e+01, 2.26420780e+01, 2.63177523e+01, 4.67571703e+01,\n",
       "       4.30773755e+01, 6.09533909e+00, 8.65046547e+00, 2.34004053e+00,\n",
       "       7.75149142e+01, 5.47665407e+00, 3.53242012e+01, 3.94416000e+01,\n",
       "       1.20021433e+01, 5.08914221e+01, 1.69083294e+01, 7.21352016e+01,\n",
       "       3.14119588e+01, 1.27273819e+01, 1.51709553e+01, 3.28223207e+00,\n",
       "       2.02674189e+01, 3.30495737e+01, 2.90791084e+01, 1.83373582e+01,\n",
       "       3.96377487e+01, 1.18991829e+01, 1.75072170e+01, 3.27093480e+01,\n",
       "       1.71017892e+01, 1.13501037e+01, 2.30483093e+01, 1.77691891e+01,\n",
       "       2.19234709e+01, 3.41787100e+01, 2.27959070e+01, 1.63564010e+01,\n",
       "       2.16242032e+01, 2.00398992e+01, 2.27976789e+01, 1.37785223e+01,\n",
       "       2.75007793e+01, 1.99154628e+01, 2.47836148e+01, 1.73359633e+01,\n",
       "       3.77560599e+01, 3.32603395e+01, 9.43594512e+00, 1.84773780e+00,\n",
       "       1.88979225e+01, 7.90670048e+00, 1.69760930e+01, 9.18546172e+00,\n",
       "       4.08150166e+00, 7.06583292e+00, 1.65520539e+01, 2.38350165e+01,\n",
       "       2.57434287e+01, 2.00693898e+01, 1.70128236e+01, 1.61522492e+01,\n",
       "       1.88784441e+01, 2.72757751e+01, 1.66588056e+01, 1.74104882e+01,\n",
       "       1.59328786e+00, 1.95905369e+01, 1.79561618e+01, 2.10138204e+01,\n",
       "       6.72197104e+00, 3.26447628e+00, 1.82650146e+01, 2.34998389e+01,\n",
       "       2.16200609e+01, 1.44831472e+01, 1.33251499e+01, 2.80280149e+01,\n",
       "       1.61728870e+01, 3.04090316e+01, 1.62718422e+01, 1.51427223e+01,\n",
       "       1.12671499e+01, 2.14837982e+01, 3.78711938e+00, 1.71447893e+01,\n",
       "       1.04541827e+01, 6.93137464e-01, 6.85182043e+00, 3.06271582e+01,\n",
       "       1.57261540e+01, 1.64410479e+01, 2.01587368e+01, 1.33061087e+01,\n",
       "       1.88206552e+01, 5.79313705e+00, 2.25597395e+01, 4.24975766e+00,\n",
       "       2.83164783e+01, 1.53083268e+01, 1.15711100e+01, 2.14553190e+01,\n",
       "       1.74982841e+01, 1.55779971e+01, 2.35028412e+01, 1.29251645e+01,\n",
       "       1.64479824e+01, 2.85773772e+01, 1.39327147e+01, 1.35916819e+01,\n",
       "       2.15628345e+01, 3.05988048e+00, 1.48873844e+01, 3.21075227e+01,\n",
       "       1.43648492e+01, 2.30096056e+01, 1.84231554e+01, 2.05799561e+01,\n",
       "       1.19368553e+01, 1.43817415e+01, 3.57024924e+01, 5.21122737e+00,\n",
       "       4.32578101e+00, 5.58205288e+00, 2.17260629e+01, 1.01060723e+01,\n",
       "       5.75620721e+01, 1.32101550e+01, 9.83807090e+00, 2.37039997e+01,\n",
       "       4.68488544e+00, 2.60621562e+01, 1.27955307e+01, 9.87549460e+00,\n",
       "       7.49914674e+00, 1.39262986e+01, 1.74999981e+01, 1.43782552e+01,\n",
       "       5.20508520e+00, 8.88288366e+01, 4.37138446e+00, 2.95175539e+01,\n",
       "       7.02043759e+00, 9.73786277e+00, 1.67708733e+01, 9.74149792e-01,\n",
       "       1.95772805e+00, 2.78070601e+01, 3.53020193e+00, 1.13174313e+00,\n",
       "       2.21542397e+01, 1.31589185e+01, 1.26880042e+01, 2.25377429e+01,\n",
       "       2.07157375e+01, 3.69932145e+00, 1.76982382e+01, 1.48879151e+01,\n",
       "       1.41986224e+01, 2.61569259e-02, 1.00518692e+01, 1.05390989e+01,\n",
       "       3.29004360e+00, 1.28611323e+01, 7.23314103e+00, 4.50115231e+00,\n",
       "       3.00425481e+01, 8.94813453e+00, 3.61395081e+00, 6.15020170e+00,\n",
       "       1.95089564e+00, 1.75907381e+01, 3.02375834e+00, 3.95161943e+00,\n",
       "       1.62754419e+00, 3.96588971e+01, 1.28553912e+01, 2.11576445e+01,\n",
       "       1.05819956e+01, 5.43561186e+00, 6.55970550e+00, 3.29203773e+00,\n",
       "       1.59976574e+00, 1.80076513e+01, 3.02465067e+01, 1.34754760e+01,\n",
       "       4.56557464e+00, 7.09127007e+00, 1.96484112e+01, 5.39837312e+00,\n",
       "       9.50048390e+00, 1.81240545e+00, 6.17561643e+00, 7.01861804e+00,\n",
       "       5.29068353e+00, 2.41394394e+01, 2.05044185e+01, 3.33663123e+00,\n",
       "       7.62600521e+00, 1.23968055e+01, 5.68709847e+00, 1.31685828e+01,\n",
       "       7.70719278e+00, 4.65233947e+00, 4.88642760e+00, 5.17864922e+00,\n",
       "       6.26743790e-01, 8.21621408e+00, 1.43762124e+00, 4.24121070e+01,\n",
       "       9.59804261e+00, 2.18554701e+00, 2.38326378e+00, 1.74247346e+01,\n",
       "       3.04777912e+00, 1.22762446e+01, 3.68372851e+01, 8.10915946e+00,\n",
       "       1.18216929e+01, 3.92364798e+00, 9.46611215e+00, 3.26456748e+00,\n",
       "       7.03684942e-01, 1.10425509e+01, 7.90091851e+00, 1.19473279e+00,\n",
       "       6.85958280e+00, 3.78439485e+00, 1.27669448e+01, 3.25673800e+00,\n",
       "       2.62369831e+00, 1.35099485e+00, 1.16857362e+01, 7.93100912e+00,\n",
       "       3.11997448e+00, 7.56870471e-01, 4.36058846e+00, 3.09555325e+00,\n",
       "       1.96140873e+00, 3.46864117e+00, 4.92492235e-01, 8.36625635e+00,\n",
       "       1.93207700e+00, 9.74037753e+00, 2.89308690e+00, 1.03206281e+01,\n",
       "       6.87593537e+00, 1.43715298e+00, 7.89477442e+00, 6.30302415e+00,\n",
       "       1.59047843e+00, 3.81511016e+00, 1.68936933e+00, 1.54770536e+00,\n",
       "       1.10451988e+01, 9.05841890e+00, 8.54414730e+00, 3.07084806e+00,\n",
       "       6.02769170e+00, 2.26279898e+00, 1.07287924e+01, 1.45271856e+01,\n",
       "       4.32299292e-01, 2.55593828e+00, 1.91698568e+00, 2.49120440e+00,\n",
       "       1.32372237e+00, 2.92061388e+00, 6.55310223e+00, 2.51040062e+00,\n",
       "       6.94059003e+00, 5.38312696e+00, 1.62809413e+00, 2.04605824e+00,\n",
       "       2.00516012e+00, 1.26884651e+01, 4.81842258e+00, 5.41489760e+00,\n",
       "       1.36174036e+01, 2.75354892e+00, 2.51516109e+00, 1.60706585e+01,\n",
       "       8.55302941e-01, 4.40900036e+00, 6.20943773e+00, 1.82305786e+00,\n",
       "       2.71002748e+00, 2.54574190e+00, 7.62803305e-01, 1.34200585e+01,\n",
       "       1.59646311e+00, 1.05519943e+01, 1.49621502e+00, 7.92304811e-01,\n",
       "       1.71585188e+00, 2.52008552e-01, 9.20056249e-01, 1.06439966e+00,\n",
       "       1.50476311e+00, 3.55214587e-01, 2.92956897e-01, 4.72718721e-01,\n",
       "       9.19026080e-01, 9.01755135e-01])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_theory = df['y-theory'].values\n",
    "df = df.drop(columns=['y-theory'])\n",
    "y_theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = train_test_split(df, test_size=0.25, random_state=42, shuffle=True)\n",
    "# test, val = train_test_split(test, test_size=0.4, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = train['y-exp'].values\n",
    "# excluded_columns = [\"Name\"]\n",
    "# X_train = train.drop(excluded_columns, axis=1)\n",
    "\n",
    "# y_test = test['y-exp'].values\n",
    "# excluded_columns = [\"Name\"]\n",
    "# X_test = test.drop(excluded_columns, axis=1)\n",
    "\n",
    "# y_val = test['y-exp'].values\n",
    "# excluded_columns = [\"Name\"]\n",
    "# X_val = test.drop(excluded_columns, axis=1)\n",
    "\n",
    "# X_train.shape[1], X_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape, y_test.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = RobustScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "# X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = torch.from_numpy(y_train)\n",
    "# y_test = torch.from_numpy(y_test)\n",
    "# y_val = torch.from_numpy(y_val)\n",
    "# #y_train = y_train.view(y_train.shape[0], 1)\n",
    "# y_test = y_test.view(y_test.shape[0], 1)\n",
    "# y_val = y_val.view(y_val.shape[0], 1)\n",
    "# y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = torch.from_numpy(X_train)\n",
    "# X_test = torch.from_numpy(X_test)\n",
    "# X_val = torch.from_numpy(X_val)\n",
    "# X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = Data.TensorDataset(X_train, y_train)\n",
    "# train_loader = Data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
    "# len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #X_train = Variable(X_train).to(device)\n",
    "# X_test = Variable(X_test).to(device)\n",
    "# X_val = Variable(X_val).to(device)\n",
    "# #y_train = Variable(y_train).to(device)\n",
    "# y_test = Variable(y_test).to(device)\n",
    "# y_val = Variable(y_val).to(device)\n",
    "# X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IRNet, self).__init__()\n",
    "        \n",
    "        self.linearXto1024 = nn.Sequential(\n",
    "            nn.Linear(X_train.shape[1], 1024, bias=True),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear1024to1024 = nn.Sequential(\n",
    "            nn.Linear(1024, 1024, bias=True),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear1024to512 = nn.Sequential(\n",
    "            nn.Linear(1024, 512, bias=True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear512to512 = nn.Sequential(\n",
    "            nn.Linear(512, 512, bias=True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear512to256 = nn.Sequential(\n",
    "            nn.Linear(512, 256, bias=True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear256to256 = nn.Sequential(\n",
    "            nn.Linear(256, 256, bias=True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear256to128 = nn.Sequential(\n",
    "            nn.Linear(256, 128, bias=True),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear128to128 = nn.Sequential(\n",
    "            nn.Linear(128, 128, bias=True),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear128to64 = nn.Sequential(\n",
    "            nn.Linear(128, 64, bias=True),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear64to64 = nn.Sequential(\n",
    "            nn.Linear(64, 64, bias=True),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear64to32 = nn.Sequential(\n",
    "            nn.Linear(64, 32, bias=True),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear32to32 = nn.Sequential(\n",
    "            nn.Linear(32, 32, bias=True),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear32to1 = nn.Linear(32, 1, bias=True)\n",
    "        \n",
    "        self.projectionXto1024 = nn.Linear(X_train.shape[1], 1024, bias=True)\n",
    "        self.projection1024to512 = nn.Linear(1024, 512, bias=True)\n",
    "        self.projection512to256 = nn.Linear(512, 256, bias=True)\n",
    "        self.projection256to128 = nn.Linear(256, 128, bias=True)\n",
    "        self.projection128to64 = nn.Linear(128, 64, bias=True)\n",
    "        self.projection64to32 = nn.Linear(64, 32, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outPrev = x\n",
    "        \n",
    "        out = self.linearXto1024(x)\n",
    "        out = torch.add(out, self.projectionXto1024(outPrev))\n",
    "        outPrev = out\n",
    "        \n",
    "        out = self.linear1024to1024(out)\n",
    "        out = torch.add(out, outPrev)\n",
    "        outPrev = out\n",
    "        out = self.linear1024to1024(out)\n",
    "        out = torch.add(out, outPrev)\n",
    "        outPrev = out\n",
    "        out = self.linear1024to1024(out)\n",
    "        out = torch.add(out, outPrev)\n",
    "        outPrev = out\n",
    "        \n",
    "        out = self.linear1024to512(out)\n",
    "        out = torch.add(out, self.projection1024to512(outPrev))\n",
    "        outPrev = out\n",
    "        \n",
    "        out = self.linear512to512(out)\n",
    "        out = torch.add(out, outPrev)\n",
    "        outPrev = out\n",
    "        out = self.linear512to512(out)\n",
    "        out = torch.add(out, outPrev)\n",
    "        outPrev = out\n",
    "        \n",
    "        out = self.linear512to256(out)\n",
    "        out = torch.add(out, self.projection512to256(outPrev))\n",
    "        outPrev = out\n",
    "        \n",
    "        out = self.linear256to256(out)\n",
    "        out = torch.add(out, outPrev)\n",
    "        outPrev = out\n",
    "        out = self.linear256to256(out)\n",
    "        out = torch.add(out, outPrev)\n",
    "        outPrev = out\n",
    "        \n",
    "        out = self.linear256to128(out)\n",
    "        out = torch.add(out, self.projection256to128(outPrev))\n",
    "        outPrev = out\n",
    "        \n",
    "        out = self.linear128to128(out)\n",
    "        out = torch.add(out, outPrev)\n",
    "        outPrev = out\n",
    "        out = self.linear128to128(out)\n",
    "        out = torch.add(out, outPrev)\n",
    "        outPrev = out\n",
    "        \n",
    "        out = self.linear128to64(out)\n",
    "        out = torch.add(out, self.projection128to64(outPrev))\n",
    "        outPrev = out\n",
    "        \n",
    "        out = self.linear64to64(out)\n",
    "        out = torch.add(out, outPrev)\n",
    "        outPrev = out\n",
    "        \n",
    "        out = self.linear64to32(out)\n",
    "        out = torch.add(out, self.projection64to32(outPrev))\n",
    "        \n",
    "        out = self.linear32to1(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "irnet = IRNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linearXto1024.0.weight torch.Size([1024, 49])\n",
      "linearXto1024.0.bias torch.Size([1024])\n",
      "linearXto1024.1.weight torch.Size([1024])\n",
      "linearXto1024.1.bias torch.Size([1024])\n",
      "linearXto1024.1.running_mean torch.Size([1024])\n",
      "linearXto1024.1.running_var torch.Size([1024])\n",
      "linearXto1024.1.num_batches_tracked torch.Size([])\n",
      "linear1024to1024.0.weight torch.Size([1024, 1024])\n",
      "linear1024to1024.0.bias torch.Size([1024])\n",
      "linear1024to1024.1.weight torch.Size([1024])\n",
      "linear1024to1024.1.bias torch.Size([1024])\n",
      "linear1024to1024.1.running_mean torch.Size([1024])\n",
      "linear1024to1024.1.running_var torch.Size([1024])\n",
      "linear1024to1024.1.num_batches_tracked torch.Size([])\n",
      "linear1024to512.0.weight torch.Size([512, 1024])\n",
      "linear1024to512.0.bias torch.Size([512])\n",
      "linear1024to512.1.weight torch.Size([512])\n",
      "linear1024to512.1.bias torch.Size([512])\n",
      "linear1024to512.1.running_mean torch.Size([512])\n",
      "linear1024to512.1.running_var torch.Size([512])\n",
      "linear1024to512.1.num_batches_tracked torch.Size([])\n",
      "linear512to512.0.weight torch.Size([512, 512])\n",
      "linear512to512.0.bias torch.Size([512])\n",
      "linear512to512.1.weight torch.Size([512])\n",
      "linear512to512.1.bias torch.Size([512])\n",
      "linear512to512.1.running_mean torch.Size([512])\n",
      "linear512to512.1.running_var torch.Size([512])\n",
      "linear512to512.1.num_batches_tracked torch.Size([])\n",
      "linear512to256.0.weight torch.Size([256, 512])\n",
      "linear512to256.0.bias torch.Size([256])\n",
      "linear512to256.1.weight torch.Size([256])\n",
      "linear512to256.1.bias torch.Size([256])\n",
      "linear512to256.1.running_mean torch.Size([256])\n",
      "linear512to256.1.running_var torch.Size([256])\n",
      "linear512to256.1.num_batches_tracked torch.Size([])\n",
      "linear256to256.0.weight torch.Size([256, 256])\n",
      "linear256to256.0.bias torch.Size([256])\n",
      "linear256to256.1.weight torch.Size([256])\n",
      "linear256to256.1.bias torch.Size([256])\n",
      "linear256to256.1.running_mean torch.Size([256])\n",
      "linear256to256.1.running_var torch.Size([256])\n",
      "linear256to256.1.num_batches_tracked torch.Size([])\n",
      "linear256to128.0.weight torch.Size([128, 256])\n",
      "linear256to128.0.bias torch.Size([128])\n",
      "linear256to128.1.weight torch.Size([128])\n",
      "linear256to128.1.bias torch.Size([128])\n",
      "linear256to128.1.running_mean torch.Size([128])\n",
      "linear256to128.1.running_var torch.Size([128])\n",
      "linear256to128.1.num_batches_tracked torch.Size([])\n",
      "linear128to128.0.weight torch.Size([128, 128])\n",
      "linear128to128.0.bias torch.Size([128])\n",
      "linear128to128.1.weight torch.Size([128])\n",
      "linear128to128.1.bias torch.Size([128])\n",
      "linear128to128.1.running_mean torch.Size([128])\n",
      "linear128to128.1.running_var torch.Size([128])\n",
      "linear128to128.1.num_batches_tracked torch.Size([])\n",
      "linear128to64.0.weight torch.Size([64, 128])\n",
      "linear128to64.0.bias torch.Size([64])\n",
      "linear128to64.1.weight torch.Size([64])\n",
      "linear128to64.1.bias torch.Size([64])\n",
      "linear128to64.1.running_mean torch.Size([64])\n",
      "linear128to64.1.running_var torch.Size([64])\n",
      "linear128to64.1.num_batches_tracked torch.Size([])\n",
      "linear64to64.0.weight torch.Size([64, 64])\n",
      "linear64to64.0.bias torch.Size([64])\n",
      "linear64to64.1.weight torch.Size([64])\n",
      "linear64to64.1.bias torch.Size([64])\n",
      "linear64to64.1.running_mean torch.Size([64])\n",
      "linear64to64.1.running_var torch.Size([64])\n",
      "linear64to64.1.num_batches_tracked torch.Size([])\n",
      "linear64to32.0.weight torch.Size([32, 64])\n",
      "linear64to32.0.bias torch.Size([32])\n",
      "linear64to32.1.weight torch.Size([32])\n",
      "linear64to32.1.bias torch.Size([32])\n",
      "linear64to32.1.running_mean torch.Size([32])\n",
      "linear64to32.1.running_var torch.Size([32])\n",
      "linear64to32.1.num_batches_tracked torch.Size([])\n",
      "linear32to32.0.weight torch.Size([32, 32])\n",
      "linear32to32.0.bias torch.Size([32])\n",
      "linear32to32.1.weight torch.Size([32])\n",
      "linear32to32.1.bias torch.Size([32])\n",
      "linear32to32.1.running_mean torch.Size([32])\n",
      "linear32to32.1.running_var torch.Size([32])\n",
      "linear32to32.1.num_batches_tracked torch.Size([])\n",
      "linear32to1.weight torch.Size([1, 32])\n",
      "linear32to1.bias torch.Size([1])\n",
      "projectionXto1024.weight torch.Size([1024, 49])\n",
      "projectionXto1024.bias torch.Size([1024])\n",
      "projection1024to512.weight torch.Size([512, 1024])\n",
      "projection1024to512.bias torch.Size([512])\n",
      "projection512to256.weight torch.Size([256, 512])\n",
      "projection512to256.bias torch.Size([256])\n",
      "projection256to128.weight torch.Size([128, 256])\n",
      "projection256to128.bias torch.Size([128])\n",
      "projection128to64.weight torch.Size([64, 128])\n",
      "projection128to64.bias torch.Size([64])\n",
      "projection64to32.weight torch.Size([32, 64])\n",
      "projection64to32.bias torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for name, param in irnet.state_dict().items():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "learning_rate = 0.001\n",
    "#criterion = nn.L1Loss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(irnet.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses_train = []\n",
    "# losses_val = []\n",
    "# epochs = []\n",
    "# minLoss = 100.0\n",
    "# minOutput = torch.Tensor(X_train.shape[0], 1)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = irnet(X_train.float())\n",
    "#     val_outputs = irnet(X_val.float())\n",
    "#     #print(type(outputs), outputs.shape)\n",
    "#     #print(outputs)\n",
    "#     #print(final_output)\n",
    "#     #print(outputs.shape)\n",
    "#     #print(ytor.shape)\n",
    "#     loss = criterion(outputs, y_train.float())\n",
    "#     if loss.data < minLoss:\n",
    "#         minLoss = loss.data\n",
    "#         minOutput = outputs\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     print('Epoch [%d/%d], Loss: %.4f'\n",
    "#          %(epoch+1, num_epochs, loss.data))\n",
    "#     epochs.append(epoch)\n",
    "#     losses_train.append(loss)\n",
    "#     losses_val.append(criterion(val_outputs, y_val.float()))\n",
    "# #print(minLoss.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(losses_train)):\n",
    "#     losses_train[i] = losses_train[i].cpu().detach().numpy()\n",
    "#     losses_val[i] = losses_val[i].cpu().detach().numpy()\n",
    "# #     losses_train[i] = losses_train[i]\n",
    "# #     losses_val[i] = losses_val[i]\n",
    "# plt.plot(epochs, losses_train)\n",
    "# plt.plot(epochs, losses_val)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_output_train = irnet(X_train.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Results on training set using 17 layer IRNet after %d iterations:\\n------------------------------'%(num_epochs))\n",
    "# print('r2 score: ', r2_score(y_train.cpu().detach().numpy(), final_output_train.cpu().detach().numpy()))\n",
    "# print('Mean Absolute Error:', criterion(final_output_train, y_train.float()).cpu().detach().numpy())\n",
    "# print('Mean Squared Error: ', mean_squared_error(y_train.cpu().detach().numpy(), final_output_train.cpu().detach().numpy()))\n",
    "# print('Root mean squared error: ', np.sqrt(mean_squared_error(y_train.cpu().detach().numpy(), final_output_train.cpu().detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_output_test = irnet(X_test.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Results on test set using 17 layer IRNet after %d iterations:\\n------------------------------'%(num_epochs))\n",
    "# print('Mean Absolute Error:', criterion(final_output_test, y_test.float()).cpu().detach().numpy())\n",
    "# print('r2 score: ', r2_score(y_test.cpu().detach().numpy(), final_output_test.cpu().detach().numpy()))\n",
    "# print('Mean Squared Error: ', mean_squared_error(y_test.cpu().detach().numpy(), final_output_test.cpu().detach().numpy()))\n",
    "# print('Root mean squared error: ', np.sqrt(mean_squared_error(y_test.cpu().detach().numpy(), final_output_test.cpu().detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-1:\n",
      " ---------------\n",
      "Epoch [1/2], Train loss: 660.9600, Validation loss: 11626.5156\n",
      "Epoch [2/2], Train loss: 3512.9434, Validation loss: 10563.0020\n",
      "lowest validation error achieved : 10563.0020\n",
      "test error:  10371.513960405708\n",
      "CV-2:\n",
      " ---------------\n",
      "Epoch [1/2], Train loss: 491.4257, Validation loss: 1734.3922\n",
      "Epoch [2/2], Train loss: 1138269.1250, Validation loss: 1318.3152\n",
      "lowest validation error achieved : 1318.3152\n",
      "test error:  1052.1211050497418\n",
      "CV-3:\n",
      " ---------------\n",
      "Epoch [1/2], Train loss: 946281.8125, Validation loss: 90117.9453\n",
      "Epoch [2/2], Train loss: 432.4896, Validation loss: 7439.0806\n",
      "lowest validation error achieved : 7439.0806\n",
      "test error:  6048.550028514858\n",
      "CV-4:\n",
      " ---------------\n",
      "Epoch [1/2], Train loss: 12088.0166, Validation loss: 3948.9343\n",
      "Epoch [2/2], Train loss: 17663.6914, Validation loss: 2566.2417\n",
      "lowest validation error achieved : 2566.2417\n",
      "test error:  2506.0094319379773\n",
      "CV-5:\n",
      " ---------------\n",
      "Epoch [1/2], Train loss: 30183.6289, Validation loss: 425.1469\n",
      "Epoch [2/2], Train loss: 2783.4094, Validation loss: 1248.8428\n",
      "lowest validation error achieved : 425.1469\n",
      "test error:  602.3941462551942\n",
      "CV-6:\n",
      " ---------------\n",
      "Epoch [1/2], Train loss: 17789.1465, Validation loss: 10412.3516\n",
      "Epoch [2/2], Train loss: 105.2784, Validation loss: 9168.5879\n",
      "lowest validation error achieved : 9168.5879\n",
      "test error:  8968.915241369004\n",
      "CV-7:\n",
      " ---------------\n",
      "Epoch [1/2], Train loss: 8062.3726, Validation loss: 744.4089\n",
      "Epoch [2/2], Train loss: 4127.5337, Validation loss: 11613.2412\n",
      "lowest validation error achieved : 744.4089\n",
      "test error:  708.5807485849283\n",
      "CV-8:\n",
      " ---------------\n",
      "Epoch [1/2], Train loss: 179.3761, Validation loss: 1358.1493\n",
      "Epoch [2/2], Train loss: 898.8599, Validation loss: 1684.0895\n",
      "lowest validation error achieved : 1358.1493\n",
      "test error:  1290.130223819236\n",
      "CV-9:\n",
      " ---------------\n",
      "Epoch [1/2], Train loss: 164.9418, Validation loss: 207204.1094\n",
      "Epoch [2/2], Train loss: 23534.3984, Validation loss: 242158.4844\n",
      "lowest validation error achieved : 207204.1094\n",
      "test error:  212721.8212166933\n",
      "CV-10:\n",
      " ---------------\n",
      "Epoch [1/2], Train loss: 661029.6875, Validation loss: 4752.5376\n",
      "Epoch [2/2], Train loss: 12090.6543, Validation loss: 137177.0156\n",
      "lowest validation error achieved : 4752.5376\n",
      "test error:  8024.885371110982\n"
     ]
    }
   ],
   "source": [
    "losses_train = []\n",
    "losses_val = []\n",
    "epochs = np.arange(1, num_epochs + 1)\n",
    "final_output_test = torch.Tensor(X_test.shape[0], 1)\n",
    "cv_error = []\n",
    "for r in range(10):\n",
    "    minLoss = 1000000000000.0\n",
    "    \n",
    "    train, test = train_test_split(df, test_size=0.25, random_state=r, shuffle=True)\n",
    "    test, val = train_test_split(test, test_size=0.4, random_state=r, shuffle=True)\n",
    "    \n",
    "    y_train = train['y-exp'].values\n",
    "    excluded_columns = [\"Name\"]\n",
    "    X_train = train.drop(excluded_columns, axis=1)\n",
    "\n",
    "    y_test = test['y-exp'].values\n",
    "    excluded_columns = [\"Name\"]\n",
    "    X_test = test.drop(excluded_columns, axis=1)\n",
    "\n",
    "    y_val = test['y-exp'].values\n",
    "    excluded_columns = [\"Name\"]\n",
    "    X_val = test.drop(excluded_columns, axis=1)\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    \n",
    "    y_train = torch.from_numpy(y_train)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "    y_val = torch.from_numpy(y_val)\n",
    "    #y_train = y_train.view(y_train.shape[0], 1)\n",
    "    y_test = y_test.view(y_test.shape[0], 1)\n",
    "    y_val = y_val.view(y_val.shape[0], 1)\n",
    "    \n",
    "    X_train = torch.from_numpy(X_train)\n",
    "    X_test = torch.from_numpy(X_test)\n",
    "    X_val = torch.from_numpy(X_val)\n",
    "    \n",
    "    train_dataset = Data.TensorDataset(X_train, y_train)\n",
    "    train_loader = Data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
    "    \n",
    "    #X_train = Variable(X_train).to(device)\n",
    "    X_test = Variable(X_test).to(device)\n",
    "    X_val = Variable(X_val).to(device)\n",
    "    #y_train = Variable(y_train).to(device)\n",
    "    y_test = Variable(y_test).to(device)\n",
    "    y_val = Variable(y_val).to(device)\n",
    "    \n",
    "    print(f'CV-{r + 1}:\\n----------------')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (batch_X, batch_y) in enumerate(train_loader):\n",
    "            #print(batch_X, batch_y)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            Xtor = Variable(batch_X).to(device)\n",
    "            ytor = Variable(batch_y.view(batch_y.shape[0], 1)).to(device)\n",
    "            #print(Xtor.shape, ytor.shape)\n",
    "\n",
    "            outputs = irnet(Xtor.float())\n",
    "            val_outputs = irnet(X_val.float())\n",
    "\n",
    "            loss = criterion(outputs, ytor.float())\n",
    "            val_loss = criterion(val_outputs, y_val.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('Epoch [%d/%d], Train loss: %.4f, Validation loss: %.4f'\n",
    "             %(epoch+1, num_epochs, loss.data, val_loss.data))\n",
    "        \n",
    "        if r == 0:\n",
    "            losses_train.append(loss)\n",
    "            losses_val.append(val_loss)\n",
    "\n",
    "        if val_loss.data < minLoss:\n",
    "            minLoss = val_loss.data\n",
    "            #print('%.4f'%(minLoss))\n",
    "            final_output_test = irnet(X_test.float())\n",
    "\n",
    "    print('lowest validation error achieved : %.4f'%(minLoss))\n",
    "    test_mse = mean_squared_error(y_test.cpu().detach().numpy(), final_output_test.cpu().detach().numpy())\n",
    "    cv_error.append(test_mse)\n",
    "    print('test error: ', test_mse)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV error :  25229.49214737409\n"
     ]
    }
   ],
   "source": [
    "print(\"CV error : \", np.mean(cv_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-0003861d61e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mlosses_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mlosses_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     losses_train[i] = losses_train[i]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     losses_val[i] = losses_val[i]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "for i in range(len(losses_train)):\n",
    "    losses_train[i] = losses_train[i].cpu().detach().numpy()\n",
    "    losses_val[i] = losses_val[i].cpu().detach().numpy()\n",
    "#     losses_train[i] = losses_train[i]\n",
    "#     losses_val[i] = losses_val[i]\n",
    "plt.plot(epochs, losses_train)\n",
    "plt.plot(epochs, losses_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Results on test set after %d iterations:\\n------------------------------------------'%(num_epochs))\n",
    "# #print('Mean Absolute Error:', criterion(final_output_test, y_test.float()).cpu().detach().numpy())\n",
    "# #print('r2 score: ', r2_score(y_test.cpu().detach().numpy(), final_output_test.cpu().detach().numpy()))\n",
    "# print('Mean Squared Error: ', mean_squared_error(y_test.cpu().detach().numpy(), final_output_test.cpu().detach().numpy()))\n",
    "# #print('Root mean squared error: ', np.sqrt(mean_squared_error(y_test.cpu().detach().numpy(), final_output_test.cpu().detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "sns.set_theme(color_codes=True)\n",
    "ax = sns.regplot(x=y_test.cpu().detach().numpy(), y=final_output_test.cpu().detach().numpy(), line_kws={\"color\": \"black\"})\n",
    "plt.xlabel('targets')\n",
    "plt.ylabel('predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses_train = []\n",
    "# losses_val = []\n",
    "# epochs = []\n",
    "# minLoss = 1000000000000.0\n",
    "# final_output_test = torch.Tensor(X_test.shape[0], 1)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     for i, (batch_X, batch_y) in enumerate(train_loader):\n",
    "#         #print(batch_X, batch_y)\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         Xtor = Variable(batch_X).to(device)\n",
    "#         ytor = Variable(batch_y.view(batch_y.shape[0], 1)).to(device)\n",
    "#         #print(Xtor.shape, ytor.shape)\n",
    "        \n",
    "#         outputs = irnet(Xtor.float())\n",
    "#         val_outputs = irnet(X_val.float())\n",
    "        \n",
    "#         loss = criterion(outputs, ytor.float())\n",
    "#         val_loss = criterion(val_outputs, y_val.float())\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print('Epoch [%d/%d], Train loss: %.4f, Validation loss: %.4f'\n",
    "#          %(epoch+1, num_epochs, loss.data, val_loss.data))\n",
    "#     epochs.append(epoch)\n",
    "#     losses_train.append(loss)\n",
    "#     losses_val.append(val_loss)\n",
    "    \n",
    "#     if val_loss.data < minLoss:\n",
    "#         minLoss = val_loss.data\n",
    "#         #print('%.4f'%(minLoss))\n",
    "#         final_output_test = irnet(X_test.float())\n",
    "\n",
    "# print('lowest validation error achieved : %.4f'%(minLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
